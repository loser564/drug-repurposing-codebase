{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f42387a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Standard Library\n",
    "# ================================\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "# ================================\n",
    "# Third-Party Libraries\n",
    "# ================================\n",
    "import bitsandbytes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import yaml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "# HuggingFace Transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback,\n",
    "    GenerationConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    __version__ as HF_VER,\n",
    ")\n",
    "\n",
    "# PEFT (LoRA / QLoRA)\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    TaskType,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "\n",
    "# TRL (Supervised Fine-Tuning Trainer)\n",
    "from trl import SFTConfig, SFTTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8ffe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(yaml_path):\n",
    "    with open(yaml_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "    \n",
    "file_paths = load_config(yaml_path=\"P3-config.yaml\")\n",
    "PARAMS = load_config(yaml_path=\"training-params.yaml\")\n",
    "prompts = load_config(yaml_path=\"prompts.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a5ee3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "drug_data_path = file_paths[\"input_file_paths\"][\"drug_data_path\"]\n",
    "\n",
    "drug_data_df = pd.read_csv(drug_data_path)\n",
    "drug_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4878cfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "summaries_path = file_paths[\"input_file_paths\"][\"summaries_path\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49929ab",
   "metadata": {},
   "source": [
    "### data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ffb6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_data_clean = drug_data_df.copy()\n",
    "drug_data_clean.columns\n",
    "# print(list(drug_data_clean.columns))\n",
    "columns_to_drop = ['PMID Count', 'PMIDs']\n",
    "drug_data_clean.drop(columns=columns_to_drop, inplace=True)\n",
    "drug_data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7099e7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _norm(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", str(s).strip().upper())\n",
    "\n",
    "def _safe(s: Optional[str]) -> str:\n",
    "    return \"\" if s is None else str(s).strip()\n",
    "\n",
    "def _read_text_file(path: str) -> Optional[str]:\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            txt = f.read().strip()\n",
    "            return txt if txt else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _canon(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Lowercase; turn any run of non-alphanumerics (incl. underscores, hyphens, commas)\n",
    "    into a single space; then collapse spaces.\n",
    "    \"\"\"\n",
    "    s = str(s).lower().strip()\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \" \", s)   # underscores, hyphens, slashes -> space\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "\n",
    "def load_reports_dir(reports_dir: str) -> Dict[str, str]:\n",
    "    pat_file = re.compile(r\"^(.+?)_summary\\.txt$\", re.I)\n",
    "    pat_rm   = re.compile(r\"PMID:\\s*\\d+\\s*(?:\\n\\s*)?(?:no relevant information found\\.?)\",\n",
    "                          re.I)\n",
    "\n",
    "    out = {}\n",
    "\n",
    "    for p in glob.glob(os.path.join(reports_dir, \"*_summary.txt\")):\n",
    "        fn = os.path.basename(p)\n",
    "        m = pat_file.match(fn)\n",
    "        if not m:\n",
    "            continue\n",
    "\n",
    "        key = _canon(m.group(1))\n",
    "\n",
    "\n",
    "        txt = _read_text_file(p)\n",
    "        if not txt:\n",
    "            continue\n",
    "\n",
    "        # remove junk pmid blocks\n",
    "        txt = pat_rm.sub(\"\", txt)\n",
    "\n",
    "        # collapse lines\n",
    "        txt = re.sub(r\"\\n{3,}\", \"\\n\\n\", txt).strip()\n",
    "\n",
    "        # *** reject empty after cleaning ***\n",
    "        if not txt:\n",
    "            # optional debug: print(f\"SKIP blank: {fn}\")\n",
    "            continue\n",
    "\n",
    "        out[key] = txt\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec9e006",
   "metadata": {},
   "source": [
    "### merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbad74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reports_dict = load_reports_dir(summaries_path)\n",
    "\n",
    "drug_data_clean[\"report\"] = drug_data_clean[\"report_key\"].map(reports_dict.get)\n",
    "pat_drop_pmid = re.compile(r\"^PMID:.*$\", re.I | re.M)\n",
    "\n",
    "drug_data_clean[\"report\"] = drug_data_clean[\"report\"].apply(\n",
    "    lambda txt: pat_drop_pmid.sub(\"\", txt).strip() if isinstance(txt, str) else txt\n",
    ")\n",
    "drug_data_clean = drug_data_clean.dropna(subset=[\"report\"]).reset_index(drop=True)\n",
    "print(drug_data_clean.shape)\n",
    "\n",
    "drug_data_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dc2163",
   "metadata": {},
   "source": [
    "### split into train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f2490e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(drug_data_clean.shape)\n",
    "train_df, val_df = train_test_split(\n",
    "    drug_data_clean,\n",
    "    test_size=0.2,          # 20% test\n",
    "    random_state=42,        # reproducible\n",
    "    shuffle=True\n",
    ")\n",
    "train_df_path = file_paths[\"train_val_paths\"][\"train_path\"]\n",
    "val_df_path = file_paths[\"train_val_paths\"][\"val_path\"]\n",
    "\n",
    "train_df.to_csv(train_df_path)\n",
    "val_df.to_csv(val_df_path)\n",
    "\n",
    "print(len(train_df), len(val_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132a0cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9b9dba",
   "metadata": {},
   "source": [
    "## llm training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f782d424",
   "metadata": {},
   "source": [
    "### load model configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb8ef00",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID   = PARAMS[\"model\"][\"id\"]\n",
    "MODEL_OUTPUT_DIR = PARAMS[\"model\"][\"output_dir\"] ## save model configs here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5dd19b",
   "metadata": {},
   "source": [
    "### training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da628675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _dtype_from_str(s: str):\n",
    "    s = (s or \"\").lower()\n",
    "    if s in (\"bf16\", \"bfloat16\"):\n",
    "        return torch.bfloat16\n",
    "    if s in (\"fp16\", \"float16\", \"half\"):\n",
    "        return torch.float16\n",
    "    if s in (\"fp32\", \"float32\"):\n",
    "        return torch.float32\n",
    "    # default safe fallback\n",
    "    return torch.float16\n",
    "\n",
    "\n",
    "def load_dataframe(path: str):\n",
    "    df = pd.read_csv(path)\n",
    "    drop_cols = PARAMS[\"data\"][\"dropna_cols\"]\n",
    "    df = df.dropna(subset=drop_cols).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "class SafeDict(dict):\n",
    "    def __missing__(self, key):\n",
    "        return \"\"  # default blank for any missing placeholder\n",
    "\n",
    "def get_first_key(ex, keys):\n",
    "    \"\"\"Return first non-empty value among candidate keys from a row/example.\"\"\"\n",
    "    for k in keys:\n",
    "        if k in ex and ex[k] is not None and str(ex[k]).strip() != \"\":\n",
    "            return str(ex[k])\n",
    "    return \"\"\n",
    "\n",
    "def chat_format_map_fn(\n",
    "    ex: Dict[str, Any],\n",
    "    training_prompt: str,\n",
    "    system_prompt: str,\n",
    "    few_shots: List[Dict[str, str]],\n",
    "    tokenizer: AutoTokenizer,\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"Map a row into chat-formatted text for SFT.\"\"\"\n",
    "    vals = {\n",
    "        \"Drug\": ex.get(\"Drug\", \"\"),\n",
    "        \"Receptor\": ex.get(\"Receptor\", \"\"),\n",
    "        \"report\": ex.get(\"report\", \"\"),\n",
    "        \"PDB_ID\": ex.get(\"PDB_ID\", \"\"),\n",
    "    }\n",
    "    user_text = training_prompt.format_map(SafeDict(vals))\n",
    "\n",
    "    # few-shots (if any)\n",
    "    msgs: List[Dict[str, str]] = []\n",
    "    for shot in few_shots or []:\n",
    "        if shot and \"user\" in shot and \"assistant\" in shot:\n",
    "            msgs.append({\"role\": \"user\", \"content\": shot[\"user\"]})\n",
    "            msgs.append({\"role\": \"assistant\", \"content\": shot[\"assistant\"]})\n",
    "\n",
    "    # fold system into first user\n",
    "    first_user = f\"<<SYS>>{system_prompt}<</SYS>>\\n\\n{user_text}\"\n",
    "    msgs.append({\"role\": \"user\", \"content\": first_user})\n",
    "\n",
    "    rendered = tokenizer.apply_chat_template(\n",
    "        msgs + [{\"role\": \"assistant\", \"content\": \"\"}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    return {\"text\": rendered}\n",
    "\n",
    "\n",
    "def plain_format_map_fn(\n",
    "    ex: Dict[str, Any],\n",
    "    training_prompt: str,\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"Map a row into plain text (no chat template) for SFT.\"\"\"\n",
    "    vals = {\n",
    "        \"Drug\": ex.get(\"Drug\", \"\"),\n",
    "        \"Receptor\": ex.get(\"Receptor\", \"\"),\n",
    "        \"Disease\": ex.get(\"Disease\", \"\"),\n",
    "        \"report\": ex.get(\"report\", \"\"),\n",
    "        \"PDB_ID\": ex.get(\"PDB_ID\", \"\"),\n",
    "    }\n",
    "    text = training_prompt.format_map(SafeDict(vals))\n",
    "    return {\"text\": text}\n",
    "\n",
    "\n",
    "def build_dataset(\n",
    "    csv_path: str,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    params: Dict[str, Any],\n",
    "    prompts: Dict[str, Any],\n",
    ") -> Dataset:\n",
    "    df = load_dataframe(csv_path, params)\n",
    "\n",
    "    training_prompt = prompts[\"train_user_template\"]\n",
    "    system_prompt = prompts[\"system\"]\n",
    "    use_chat_format = prompts.get(\"use_chat_format\", False)\n",
    "    few_shots = prompts.get(\"few_shots\", [])\n",
    "\n",
    "    cols = list(df.columns)\n",
    "    ds = Dataset.from_pandas(df, preserve_index=False)\n",
    "\n",
    "    if use_chat_format:\n",
    "        # HuggingFace Datasets map passes only the example dict, so we wrap\n",
    "        # our extra arguments using a closure-like helper via kwargs.\n",
    "        def _wrapped_chat_map_fn(ex: Dict[str, Any]) -> Dict[str, str]:\n",
    "            return chat_format_map_fn(\n",
    "                ex=ex,\n",
    "                training_prompt=training_prompt,\n",
    "                system_prompt=system_prompt,\n",
    "                few_shots=few_shots,\n",
    "                tokenizer=tokenizer,\n",
    "            )\n",
    "\n",
    "        ds = ds.map(_wrapped_chat_map_fn, remove_columns=cols)\n",
    "    else:\n",
    "        def _wrapped_plain_map_fn(ex: Dict[str, Any]) -> Dict[str, str]:\n",
    "            return plain_format_map_fn(\n",
    "                ex=ex,\n",
    "                training_prompt=training_prompt,\n",
    "            )\n",
    "\n",
    "        ds = ds.map(_wrapped_plain_map_fn, remove_columns=cols)\n",
    "\n",
    "    ds = ds.train_test_split(\n",
    "        test_size=params[\"data\"][\"test_size\"],\n",
    "        seed=params[\"data\"][\"seed\"],\n",
    "    )\n",
    "    return ds\n",
    "\n",
    "\n",
    "\n",
    "def get_model_and_tokenizer(params: Dict[str, Any]):\n",
    "    # Perf niceties\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "    # --- bitsandbytes (QLoRA) config from params ---\n",
    "    compute_dtype = _dtype_from_str(params[\"bnb\"][\"compute_dtype\"])\n",
    "    bnb_cfg = BitsAndBytesConfig(\n",
    "        load_in_4bit=params[\"model\"][\"load_in_4bit\"],\n",
    "        bnb_4bit_quant_type=params[\"bnb\"][\"quant_type\"],\n",
    "        bnb_4bit_use_double_quant=params[\"bnb\"][\"double_quant\"],\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "    )\n",
    "\n",
    "    model_id = params[\"model\"][\"id\"]\n",
    "    tok = AutoTokenizer.from_pretrained(\n",
    "        model_id,\n",
    "        use_fast=True,\n",
    "        trust_remote_code=params[\"model\"][\"trust_remote_code\"],\n",
    "    )\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    tok.padding_side = \"right\"\n",
    "\n",
    "    max_memory = params[\"model\"][\"max_memory\"]\n",
    "    os.makedirs(params[\"model\"][\"offload_folder\"], exist_ok=True)\n",
    "\n",
    "    torch_dtype = _dtype_from_str(params[\"model\"][\"torch_dtype\"])\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=bnb_cfg,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch_dtype,\n",
    "        max_memory=max_memory,\n",
    "        offload_folder=params[\"model\"][\"offload_folder\"],\n",
    "        low_cpu_mem_usage=True,\n",
    "        trust_remote_code=params[\"model\"][\"trust_remote_code\"],\n",
    "    )\n",
    "    model.config.use_cache = False\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    # Prepare for k-bit training (casts norms/embeddings for stability)\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    if getattr(model.config, \"pad_token_id\", None) is None:\n",
    "        model.config.pad_token_id = tok.pad_token_id\n",
    "\n",
    "    return model, tok\n",
    "\n",
    "\n",
    "def get_lora_wrapped(model, params: Dict[str, Any]):\n",
    "    lconf = params[\"lora\"]\n",
    "    lora_cfg = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        r=lconf[\"r\"],\n",
    "        lora_alpha=lconf[\"alpha\"],\n",
    "        lora_dropout=lconf[\"dropout\"],\n",
    "        target_modules=lconf[\"target_modules\"],\n",
    "        bias=lconf[\"bias\"],\n",
    "    )\n",
    "    model = get_peft_model(model, lora_cfg)\n",
    "    return model\n",
    "\n",
    "\n",
    "def main(\n",
    "    train_csv: str,\n",
    "    file_paths: Dict[str, Any],\n",
    "    params: Dict[str, Any],\n",
    "    prompts: Dict[str, Any],\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Run SFT training.\n",
    "\n",
    "    Args:\n",
    "        train_csv: Path to the training CSV.\n",
    "        file_paths: Config dict from P3-config.yaml (if you need more paths).\n",
    "        params: Training hyperparameters and model settings.\n",
    "        prompts: Prompt templates and chat-format toggles.\n",
    "    \"\"\"\n",
    "    model, tokenizer = get_model_and_tokenizer(params)\n",
    "    model = get_lora_wrapped(model, params)\n",
    "\n",
    "    ds = build_dataset(train_csv, tokenizer, params, prompts)\n",
    "\n",
    "    training_args = SFTConfig(\n",
    "        output_dir=params[\"model\"][\"output_dir\"],\n",
    "        num_train_epochs=params[\"train\"][\"epochs\"],\n",
    "        per_device_train_batch_size=params[\"train\"][\"per_device_train_batch_size\"],\n",
    "        per_device_eval_batch_size=params[\"train\"][\"per_device_eval_batch_size\"],\n",
    "        gradient_accumulation_steps=params[\"train\"][\"gradient_accumulation_steps\"],\n",
    "        learning_rate=params[\"train\"][\"learning_rate\"],\n",
    "        lr_scheduler_type=params[\"train\"][\"scheduler\"],\n",
    "        warmup_ratio=params[\"train\"][\"warmup_ratio\"],\n",
    "        weight_decay=params[\"train\"][\"weight_decay\"],\n",
    "        logging_steps=params[\"train\"][\"logging_steps\"],\n",
    "        evaluation_strategy=params[\"train\"][\"evaluation_strategy\"],\n",
    "        eval_steps=params[\"train\"][\"eval_steps\"],\n",
    "        save_steps=params[\"train\"][\"save_steps\"],\n",
    "        save_total_limit=params[\"train\"][\"save_total_limit\"],\n",
    "        bf16=params[\"train\"][\"bf16\"],\n",
    "        fp16=params[\"train\"][\"fp16\"],\n",
    "        gradient_checkpointing=params[\"train\"][\"gradient_checkpointing\"],\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "        packing=params[\"data\"][\"packing\"],\n",
    "        optim=params[\"train\"][\"optim\"],\n",
    "        max_grad_norm=params[\"train\"][\"max_grad_norm\"],\n",
    "        seed=params[\"train\"][\"seed\"],\n",
    "        report_to=params[\"train\"][\"report_to\"],\n",
    "        load_best_model_at_end=params[\"train\"][\"load_best_model\"],\n",
    "        # SFT-specific fields\n",
    "        max_seq_length=params[\"data\"][\"max_seq_length\"],\n",
    "        dataset_text_field=\"text\",\n",
    "    )\n",
    "\n",
    "    early_stopping = EarlyStoppingCallback(\n",
    "        early_stopping_patience=1,      # small because only a few epochs\n",
    "        early_stopping_threshold=0.0,\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=ds[\"train\"],\n",
    "        eval_dataset=ds[\"test\"],\n",
    "        args=training_args,\n",
    "        callbacks=[early_stopping],\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model()\n",
    "    tokenizer.save_pretrained(params[\"model\"][\"output_dir\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c7ef54",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load configs from YAML\n",
    "    file_paths = load_config(yaml_path=\"P3-config.yaml\")\n",
    "    params = load_config(yaml_path=\"training-params.yaml\")\n",
    "    prompts = load_config(yaml_path=\"prompts.yaml\")\n",
    "\n",
    "    # Default to train_path from config, but you can pass any CSV you want\n",
    "    train_csv_path = file_paths[\"train_val_paths\"][\"train_path\"]\n",
    "\n",
    "    main(\n",
    "        train_csv=train_csv_path,\n",
    "        file_paths=file_paths,\n",
    "        params=params,\n",
    "        prompts=prompts,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
