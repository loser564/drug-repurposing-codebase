{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05069e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Standard Library\n",
    "# ================================\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "# ================================\n",
    "# Third-Party Libraries\n",
    "# ================================\n",
    "import bitsandbytes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import yaml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "# HuggingFace Transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback,\n",
    "    GenerationConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    __version__ as HF_VER,\n",
    ")\n",
    "\n",
    "# PEFT (LoRA / QLoRA)\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    TaskType,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "\n",
    "# TRL (Supervised Fine-Tuning Trainer)\n",
    "from trl import SFTConfig, SFTTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd9db4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(yaml_path):\n",
    "    with open(yaml_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "    \n",
    "file_paths = load_config(yaml_path=\"P3-config.yaml\")\n",
    "PARAMS = load_config(yaml_path=\"validation_params.yaml\")\n",
    "prompts = load_config(yaml_path=\"prompts.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0f7342a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Drug</th>\n",
       "      <th>Receptor</th>\n",
       "      <th>PDB_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46</td>\n",
       "      <td>Leucovorin Calcium</td>\n",
       "      <td>Dihydrofolate reductase</td>\n",
       "      <td>8WGN,7XI7,6DAV,5SDB,5SDA,5SD9,5SD8,5SD7,5SD6,4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>Letrozole</td>\n",
       "      <td>Aromatase</td>\n",
       "      <td>5JL9,5JL7,5JL6,5JKW,5JKV,4GL7,4GL5,3S7S,3S79,3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Adenosine triphosphate</td>\n",
       "      <td>Tyrosine-protein kinase ABL1</td>\n",
       "      <td>5MO4,1OPK,1OPL,2FO0,8SSN,4XEY,6XR7,6XR6,2E2B,4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>Dapagliflozin</td>\n",
       "      <td>Sodium/glucose cotransporter 2</td>\n",
       "      <td>7YNK,7YNJ,7VSI,8HIN,8HG7,8HEZ,8HDH,8HB0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Amphetamine</td>\n",
       "      <td>Trace amine-associated receptor-1</td>\n",
       "      <td>8JSO,8JLR,8JLQ,8JLP,8JLO,8JLN,8UHB,8ZSS,8ZSP,8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                    Drug                           Receptor  \\\n",
       "0          46      Leucovorin Calcium            Dihydrofolate reductase   \n",
       "1          16               Letrozole                          Aromatase   \n",
       "2           4  Adenosine triphosphate       Tyrosine-protein kinase ABL1   \n",
       "3           9           Dapagliflozin     Sodium/glucose cotransporter 2   \n",
       "4          28             Amphetamine  Trace amine-associated receptor-1   \n",
       "\n",
       "                                              PDB_ID  \n",
       "0  8WGN,7XI7,6DAV,5SDB,5SDA,5SD9,5SD8,5SD7,5SD6,4...  \n",
       "1  5JL9,5JL7,5JL6,5JKW,5JKV,4GL7,4GL5,3S7S,3S79,3...  \n",
       "2  5MO4,1OPK,1OPL,2FO0,8SSN,4XEY,6XR7,6XR6,2E2B,4...  \n",
       "3            7YNK,7YNJ,7VSI,8HIN,8HG7,8HEZ,8HDH,8HB0  \n",
       "4  8JSO,8JLR,8JLQ,8JLP,8JLO,8JLN,8UHB,8ZSS,8ZSP,8...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df_path = file_paths[\"train_val_paths\"][\"val_path\"]\n",
    "val_df = pd.read_csv(val_df_path)\n",
    "columns_to_drop = ['report', 'report_key']\n",
    "val_df = val_df.drop(columns=columns_to_drop) \n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b37f570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _dtype_from_str(s: str):\n",
    "    s = (s or \"\").lower()\n",
    "    if s in (\"bf16\", \"bfloat16\"):\n",
    "        return torch.bfloat16\n",
    "    if s in (\"fp16\", \"float16\", \"half\"):\n",
    "        return torch.float16\n",
    "    if s in (\"fp32\", \"float32\"):\n",
    "        return torch.float32\n",
    "    return torch.float16\n",
    "\n",
    "\n",
    "def make_bnb_config(infer_cfg: Dict[str, Any]) -> BitsAndBytesConfig:\n",
    "    \"\"\"\n",
    "    Build BitsAndBytesConfig from validation_params.yaml's `quantization` block.\n",
    "    \"\"\"\n",
    "    q = infer_cfg[\"quantization\"]\n",
    "    compute_dtype = q.get(\"compute_dtype\", \"auto\")\n",
    "\n",
    "    # allow \"auto\" in YAML, resolve here\n",
    "    if compute_dtype == \"auto\":\n",
    "        compute_dtype = \"bf16\" if torch.cuda.is_bf16_supported() else \"fp16\"\n",
    "\n",
    "    return BitsAndBytesConfig(\n",
    "        load_in_4bit=q[\"load_in_4bit\"],\n",
    "        bnb_4bit_quant_type=q[\"quant_type\"],\n",
    "        bnb_4bit_use_double_quant=q[\"double_quant\"],\n",
    "        bnb_4bit_compute_dtype=_dtype_from_str(compute_dtype),\n",
    "    )\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Model loading\n",
    "# -------------------------------\n",
    "\n",
    "def load_model_and_tokenizer_for_inference(\n",
    "    infer_cfg: Dict[str, Any]\n",
    "):\n",
    "    \"\"\"\n",
    "    Load base model + LoRA adapter for inference.\n",
    "\n",
    "    Uses:\n",
    "      - 4-bit quantization (preferred) OR\n",
    "      - merged 16-bit model if `merge_16bit` is True.\n",
    "    \"\"\"\n",
    "    base_id = infer_cfg[\"base_model_id\"]\n",
    "    adapter_dir = infer_cfg[\"adapter_dir\"]\n",
    "    device_map = infer_cfg[\"batch\"][\"device_map\"]\n",
    "    merge_16bit = infer_cfg[\"merge_16bit\"]\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        base_id,\n",
    "        use_fast=True,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    if merge_16bit:\n",
    "        # 16-bit merged model\n",
    "        dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_id,\n",
    "            device_map=device_map,\n",
    "            torch_dtype=dtype,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        model = PeftModel.from_pretrained(model, adapter_dir)\n",
    "        model = model.merge_and_unload()\n",
    "    else:\n",
    "        # 4-bit quantized path\n",
    "        bnb_cfg = make_bnb_config(infer_cfg)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_id,\n",
    "            quantization_config=bnb_cfg,\n",
    "            device_map=device_map,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        model = PeftModel.from_pretrained(model, adapter_dir)\n",
    "\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Prompt composition\n",
    "# -------------------------------\n",
    "\n",
    "\n",
    "PLACEHOLDER_RE = re.compile(r\"\\{([A-Za-z0-9_]+)\\}\")\n",
    "\n",
    "def render_template_safe(template: str, values: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Replace {Key} placeholders using a regex, leaving all other braces intact.\n",
    "    This prevents JSON examples like { \"a\": 1 } from being treated as format fields\n",
    "    because we do NOT call .format().\n",
    "    Only patterns that look like {WordLikeKey} are replaced.\n",
    "    \"\"\"\n",
    "    def repl(m):\n",
    "        key = m.group(1)\n",
    "        v = values.get(key, \"\")\n",
    "        return \"\" if v is None else str(v)\n",
    "    return PLACEHOLDER_RE.sub(repl, template or \"\")\n",
    "\n",
    "\n",
    "def compose_chat_prompt_llama32(\n",
    "    system_prompt: str,\n",
    "    user_template: str,\n",
    "    values: Dict[str, Any],\n",
    "    tokenizer,\n",
    "    few_shots: Optional[List[Dict[str, str]]] = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Llama-3.2 Instruct chat formatting:\n",
    "    - system message is its own role\n",
    "    - user message contains rendered template text\n",
    "    - apply_chat_template handles <|begin_of_text|> etc\n",
    "    \"\"\"\n",
    "    user_text = render_template_safe(user_template, values)\n",
    "\n",
    "    msgs: List[Dict[str, str]] = []\n",
    "    if system_prompt and str(system_prompt).strip():\n",
    "        msgs.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "\n",
    "    # Optional few-shot examples (same structure as your training config)\n",
    "    for shot in few_shots or []:\n",
    "        if shot and \"user\" in shot and \"assistant\" in shot:\n",
    "            msgs.append({\"role\": \"user\", \"content\": shot[\"user\"]})\n",
    "            msgs.append({\"role\": \"assistant\", \"content\": shot[\"assistant\"]})\n",
    "\n",
    "    msgs.append({\"role\": \"user\", \"content\": user_text})\n",
    "\n",
    "    # For inference, we want the assistant header/prefix to be appended\n",
    "    return tokenizer.apply_chat_template(\n",
    "        msgs,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "\n",
    "def compose_plain_prompt_safe(\n",
    "    user_template: str,\n",
    "    values: Dict[str, Any],\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Plain prompt version that is also JSON-brace-safe.\n",
    "    \"\"\"\n",
    "    return render_template_safe(user_template, values)\n",
    "\n",
    "# -------------------------------\n",
    "# Row-level generation\n",
    "# -------------------------------\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_for_row(\n",
    "    row: Dict[str, Any],\n",
    "    model,\n",
    "    tokenizer,\n",
    "    infer_cfg: Dict[str, Any],\n",
    "    prompts_cfg: Dict[str, Any],\n",
    "    test: bool,\n",
    ") -> str:\n",
    "    col_cfg = infer_cfg[\"columns\"]\n",
    "    col_d = col_cfg[\"drug\"]\n",
    "    col_r = col_cfg[\"receptor\"]\n",
    "    col_p = col_cfg[\"pdb\"]\n",
    "    col_rep = col_cfg.get(\"report\")  # OPTIONAL: add this in your validation_params.yaml if you have report\n",
    "\n",
    "    drug = str(row.get(col_d, \"\") or \"\")\n",
    "    receptor = str(row.get(col_r, \"\") or \"\")\n",
    "\n",
    "    pdb_raw = str(row.get(col_p, \"\") or \"\")\n",
    "    pdb_ids = \",\".join([p.strip() for p in pdb_raw.split(\",\") if p.strip()]) if pdb_raw else \"\"\n",
    "\n",
    "    report = \"\"\n",
    "    if col_rep:\n",
    "        report = str(row.get(col_rep, \"\") or \"\")\n",
    "\n",
    "    p_cfg = prompts_cfg.get(\"prompts\", prompts_cfg)\n",
    "    use_chat_format = p_cfg.get(\"use_chat_format\", False)\n",
    "    system_prompt = p_cfg.get(\"system\", \"\")\n",
    "    few_shots = p_cfg.get(\"few_shots\", [])\n",
    "\n",
    "    # Pick the right template for validation vs test\n",
    "    user_template = p_cfg[\"test_user_template\"] if test else p_cfg[\"val_user_template\"]\n",
    "\n",
    "    values = {\n",
    "        \"Drug\": drug,\n",
    "        \"Receptor\": receptor,\n",
    "        \"PDB_ID\": pdb_ids,\n",
    "        \"report\": report,   # IMPORTANT: supports {report} in templates\n",
    "    }\n",
    "\n",
    "    if use_chat_format:\n",
    "        prompt = compose_chat_prompt_llama32(\n",
    "            system_prompt=system_prompt,\n",
    "            user_template=user_template,\n",
    "            values=values,\n",
    "            tokenizer=tokenizer,\n",
    "            few_shots=few_shots,\n",
    "        )\n",
    "    else:\n",
    "        prompt = compose_plain_prompt_safe(\n",
    "            user_template=user_template,\n",
    "            values=values,\n",
    "        )\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    gen_cfg = infer_cfg[\"generation\"]\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=gen_cfg[\"max_new_tokens\"],\n",
    "        do_sample=gen_cfg[\"do_sample\"],\n",
    "        temperature=gen_cfg[\"temperature\"],\n",
    "        top_p=gen_cfg[\"top_p\"],\n",
    "        top_k=gen_cfg[\"top_k\"],\n",
    "        repetition_penalty=gen_cfg[\"repetition_penalty\"],\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "    generated = out[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    text = tokenizer.decode(generated, skip_special_tokens=True)\n",
    "\n",
    "    for s in gen_cfg.get(\"stop_strings\", []):\n",
    "        if s and s in text:\n",
    "            text = text.split(s)[0].rstrip()\n",
    "            break\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# -------------------------------\n",
    "# Output path helper\n",
    "# -------------------------------\n",
    "\n",
    "def get_output_path(\n",
    "    infer_cfg: Dict[str, Any],\n",
    "    test: bool,\n",
    "    relation: bool,\n",
    "    real_world: bool,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Decide which CSV path to save to based on flags.\n",
    "    Uses the `save_paths` block from validation_params.yaml.\n",
    "    \"\"\"\n",
    "    paths = infer_cfg[\"save_paths\"]\n",
    "\n",
    "    if not test:\n",
    "        return paths[\"inference_reports\"]\n",
    "\n",
    "    if real_world:\n",
    "        return paths[\"real_world_test\"]\n",
    "\n",
    "    if relation:\n",
    "        return paths[\"some_relation\"]\n",
    "\n",
    "    return paths[\"no_relation\"]\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# DataFrame-level inference\n",
    "# -------------------------------\n",
    "\n",
    "def run_inference(\n",
    "    df: pd.DataFrame,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    infer_cfg: Dict[str, Any],\n",
    "    prompts_cfg: Dict[str, Any],\n",
    "    test: bool,\n",
    "    relation: bool = False,\n",
    "    real_world: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run inference over all rows in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df: Input examples (must contain columns specified in infer_cfg[\"columns\"]).\n",
    "        model, tokenizer: Loaded via `load_model_and_tokenizer_for_inference`.\n",
    "        infer_cfg: validation_params.yaml contents.\n",
    "        prompts_cfg: prompts.yaml contents.\n",
    "        test: If False => validation-like set; if True => test scenarios.\n",
    "        relation: If True (and test=True), saves to \"some_relation\" path.\n",
    "        real_world: If True (and test=True), saves to \"real_world_test\" path.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with an extra column `generated_report`.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    for _, row in df.iterrows():\n",
    "        report = generate_for_row(\n",
    "            row=row.to_dict(),\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            infer_cfg=infer_cfg,\n",
    "            prompts_cfg=prompts_cfg,\n",
    "            test=test,\n",
    "        )\n",
    "        rec = dict(row)\n",
    "        rec[\"generated_report\"] = report\n",
    "        records.append(rec)\n",
    "\n",
    "    out_df = pd.DataFrame(records)\n",
    "    out_path = get_output_path(\n",
    "        infer_cfg=infer_cfg,\n",
    "        test=test,\n",
    "        relation=relation,\n",
    "        real_world=real_world,\n",
    "    )\n",
    "    out_df.to_csv(out_path, index=False)\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f639c528",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_model_and_tokenizer_for_inference(PARAMS)\n",
    "\n",
    "val_results_df = run_inference(\n",
    "    df=val_df,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    infer_cfg=PARAMS,\n",
    "    prompts_cfg=prompts,\n",
    "    test=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3911b14f-d863-4006-b3dc-7c777f86ed05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fypkernel2",
   "language": "python",
   "name": "fyp_kernel2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
