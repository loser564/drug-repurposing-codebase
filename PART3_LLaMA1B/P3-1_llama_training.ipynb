{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf20fdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Standard Library\n",
    "# ================================\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "# ================================\n",
    "# Third-Party Libraries\n",
    "# ================================\n",
    "import bitsandbytes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import yaml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "# HuggingFace Transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback,\n",
    "    GenerationConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    __version__ as HF_VER,\n",
    ")\n",
    "\n",
    "# PEFT (LoRA / QLoRA)\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    TaskType,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "\n",
    "# TRL (Supervised Fine-Tuning Trainer)\n",
    "from trl import SFTConfig, SFTTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34ec2cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(yaml_path):\n",
    "    with open(yaml_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "    \n",
    "file_paths = load_config(yaml_path=\"P3-config.yaml\")\n",
    "PARAMS = load_config(yaml_path=\"training_params.yaml\")\n",
    "prompts = load_config(yaml_path=\"prompts.yaml\")\n",
    "\n",
    "dtype = \"bf16\" if torch.cuda.is_bf16_supported() else \"fp16\"\n",
    "PARAMS[\"model\"][\"dtype\"] = dtype\n",
    "PARAMS[\"bnb\"][\"compute_dtype\"] = dtype\n",
    "PARAMS[\"train\"][\"bf16\"] = torch.cuda.is_bf16_supported()\n",
    "PARAMS[\"train\"][\"fp16\"] = not torch.cuda.is_bf16_supported()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e1931fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Drug</th>\n",
       "      <th>Receptor</th>\n",
       "      <th>PDB_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Metformin</td>\n",
       "      <td>Acetyl-CoA carboxylase 2</td>\n",
       "      <td>3FF6,3TDC,2X24,3JRX,3JRW,2HJW,4HQ6,5KKN,3GLK,3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cetuximab</td>\n",
       "      <td>Epidermal growth factor receptor</td>\n",
       "      <td>7SZ7,7SZ5,7SYE,7SYD,7SZ1,7SZ0,8HGS,8HGP,8HGO,5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bevacizumab</td>\n",
       "      <td>Vascular endothelial growth factor A</td>\n",
       "      <td>3V2A,5T89,8UWZ,6T9D,7KF1,7KF0,7KEZ,5FV2,5FV1,3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pioglitazone</td>\n",
       "      <td>Peroxisome proliferator-activated receptor gamma</td>\n",
       "      <td>3E00,3DZY,3DZU,7QB1,6L89,6K0T,6AD9,5HZC,5F9B,5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adenosine triphosphate</td>\n",
       "      <td>Tyrosine-protein kinase ABL1</td>\n",
       "      <td>5MO4,1OPK,1OPL,2FO0,8SSN,4XEY,6XR7,6XR6,2E2B,4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Drug                                          Receptor  \\\n",
       "0               Metformin                          Acetyl-CoA carboxylase 2   \n",
       "1               Cetuximab                  Epidermal growth factor receptor   \n",
       "2             Bevacizumab              Vascular endothelial growth factor A   \n",
       "3            Pioglitazone  Peroxisome proliferator-activated receptor gamma   \n",
       "4  Adenosine triphosphate                      Tyrosine-protein kinase ABL1   \n",
       "\n",
       "                                              PDB_ID  \n",
       "0  3FF6,3TDC,2X24,3JRX,3JRW,2HJW,4HQ6,5KKN,3GLK,3...  \n",
       "1  7SZ7,7SZ5,7SYE,7SYD,7SZ1,7SZ0,8HGS,8HGP,8HGO,5...  \n",
       "2  3V2A,5T89,8UWZ,6T9D,7KF1,7KF0,7KEZ,5FV2,5FV1,3...  \n",
       "3  3E00,3DZY,3DZU,7QB1,6L89,6K0T,6AD9,5HZC,5F9B,5...  \n",
       "4  5MO4,1OPK,1OPL,2FO0,8SSN,4XEY,6XR7,6XR6,2E2B,4...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "drug_data_path = file_paths[\"input_file_paths\"][\"drug_data_path\"]\n",
    "\n",
    "drug_data_df = pd.read_csv(drug_data_path)\n",
    "drug_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebb694ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "summaries_path = file_paths[\"input_file_paths\"][\"summaries_path\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bfab04",
   "metadata": {},
   "source": [
    "## Data cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b9c563f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Drug</th>\n",
       "      <th>Receptor</th>\n",
       "      <th>PDB_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Metformin</td>\n",
       "      <td>Acetyl-CoA carboxylase 2</td>\n",
       "      <td>3FF6,3TDC,2X24,3JRX,3JRW,2HJW,4HQ6,5KKN,3GLK,3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cetuximab</td>\n",
       "      <td>Epidermal growth factor receptor</td>\n",
       "      <td>7SZ7,7SZ5,7SYE,7SYD,7SZ1,7SZ0,8HGS,8HGP,8HGO,5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bevacizumab</td>\n",
       "      <td>Vascular endothelial growth factor A</td>\n",
       "      <td>3V2A,5T89,8UWZ,6T9D,7KF1,7KF0,7KEZ,5FV2,5FV1,3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pioglitazone</td>\n",
       "      <td>Peroxisome proliferator-activated receptor gamma</td>\n",
       "      <td>3E00,3DZY,3DZU,7QB1,6L89,6K0T,6AD9,5HZC,5F9B,5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adenosine triphosphate</td>\n",
       "      <td>Tyrosine-protein kinase ABL1</td>\n",
       "      <td>5MO4,1OPK,1OPL,2FO0,8SSN,4XEY,6XR7,6XR6,2E2B,4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Drug                                          Receptor  \\\n",
       "0               Metformin                          Acetyl-CoA carboxylase 2   \n",
       "1               Cetuximab                  Epidermal growth factor receptor   \n",
       "2             Bevacizumab              Vascular endothelial growth factor A   \n",
       "3            Pioglitazone  Peroxisome proliferator-activated receptor gamma   \n",
       "4  Adenosine triphosphate                      Tyrosine-protein kinase ABL1   \n",
       "\n",
       "                                              PDB_ID  \n",
       "0  3FF6,3TDC,2X24,3JRX,3JRW,2HJW,4HQ6,5KKN,3GLK,3...  \n",
       "1  7SZ7,7SZ5,7SYE,7SYD,7SZ1,7SZ0,8HGS,8HGP,8HGO,5...  \n",
       "2  3V2A,5T89,8UWZ,6T9D,7KF1,7KF0,7KEZ,5FV2,5FV1,3...  \n",
       "3  3E00,3DZY,3DZU,7QB1,6L89,6K0T,6AD9,5HZC,5F9B,5...  \n",
       "4  5MO4,1OPK,1OPL,2FO0,8SSN,4XEY,6XR7,6XR6,2E2B,4...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_data_clean = drug_data_df.copy()\n",
    "drug_data_clean.columns\n",
    "\n",
    "drug_data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49e1f672",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _norm(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", str(s).strip().upper())\n",
    "\n",
    "def _safe(s: Optional[str]) -> str:\n",
    "    return \"\" if s is None else str(s).strip()\n",
    "\n",
    "def _read_text_file(path: str) -> Optional[str]:\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            txt = f.read().strip()\n",
    "            return txt if txt else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _canon(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Lowercase; turn any run of non-alphanumerics (incl. underscores, hyphens, commas)\n",
    "    into a single space; then collapse spaces.\n",
    "    \"\"\"\n",
    "    s = str(s).lower().strip()\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \" \", s)   # underscores, hyphens, slashes -> space\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "\n",
    "def load_reports_dir(reports_dir: str) -> Dict[str, str]:\n",
    "    pat_file = re.compile(r\"^(.+?)_summary\\.txt$\", re.I)\n",
    "    pat_rm   = re.compile(r\"PMID:\\s*\\d+\\s*(?:\\n\\s*)?(?:no relevant information found\\.?)\",\n",
    "                          re.I)\n",
    "\n",
    "    out = {}\n",
    "\n",
    "    for p in glob.glob(os.path.join(reports_dir, \"*_summary.txt\")):\n",
    "        fn = os.path.basename(p)\n",
    "        m = pat_file.match(fn)\n",
    "        if not m:\n",
    "            continue\n",
    "\n",
    "        key = _canon(m.group(1))\n",
    "\n",
    "\n",
    "        txt = _read_text_file(p)\n",
    "        if not txt:\n",
    "            continue\n",
    "\n",
    "        # remove junk pmid blocks\n",
    "        txt = pat_rm.sub(\"\", txt)\n",
    "\n",
    "        # collapse lines\n",
    "        txt = re.sub(r\"\\n{3,}\", \"\\n\\n\", txt).strip()\n",
    "\n",
    "        # *** reject empty after cleaning ***\n",
    "        if not txt:\n",
    "            # optional debug: print(f\"SKIP blank: {fn}\")\n",
    "            continue\n",
    "\n",
    "        out[key] = txt\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b02f4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 | amphetamine\n",
      "PMID: 39287256\n",
      "The study found KEGG enrichment of shared NASH/type 2 diabetes targets in the \"amphetamine\n",
      "addiction\" pathway alongside colorectal cancer, PPAR signaling and toll‑like receptor signalin ...\n",
      "\n",
      "01 | semaglutide\n",
      "PMID: 40437949\n",
      "Semaglutide, as a GLP‑1 receptor agonist used to treat diabetes and obesity, was part of trials\n",
      "pooled here showing no overall cancer risk but a small increased colorectal cancer signal ...\n",
      "\n",
      "02 | adenosine monophosphate\n",
      "PMID: 37071615\n",
      "Patchouli alcohol (PA) treatment increased phosphorylation (activation) of 5' adenosine\n",
      "monophosphate‑activated protein kinase (AMPK) alongside protein kinase B (Akt) in differentiated\n",
      " ...\n",
      "\n",
      "03 | hydroxychloroquine\n",
      "PMID: 33608672\n",
      "The study shows that blocking autophagy with chloroquine potentiated killing of KRAS‑mutant CRC\n",
      "cells treated with glycolysis and OXPHOS inhibitors; by extension, hydroxychloroquine (a  ...\n",
      "\n",
      "04 | cimetidine\n",
      "PMID: 12938277\n",
      "In this case report of a diabetic patient with ascending colon carcinoma and lung metastases,\n",
      "postoperative oral cimetidine (800 mg/day) given with 5'-DFUR was associated with relativel ...\n",
      "\n",
      "05 | thiazolidinedione\n",
      "PMID: 31887708\n",
      "In this large population-based cohort of patients with newly diagnosed type 2 diabetes, use of\n",
      "thiazolidinediones was not associated with a change in colorectal cancer risk. The study t ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reports_dict = load_reports_dir(summaries_path)\n",
    "\n",
    "for i, (drug, text) in enumerate(reports_dict.items()):\n",
    "    print(f\"{i:02d} | {drug}\")\n",
    "    print(text[:200], \"...\\n\")      # first 200 chars preview\n",
    "\n",
    "    if i> 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59afbaa6",
   "metadata": {},
   "source": [
    "## merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53c7dd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- DRUG → report_key → exists ----\n",
      "Metformin                           → metformin                           → True\n",
      "Cetuximab                           → cetuximab                           → True\n",
      "Bevacizumab                         → bevacizumab                         → True\n",
      "Pioglitazone                        → pioglitazone                        → True\n",
      "Adenosine triphosphate              → adenosine triphosphate              → True\n",
      "Thiazolidinedione                   → thiazolidinedione                   → True\n",
      "Estrogen                            → estrogen                            → True\n",
      "Exenatide                           → exenatide                           → True\n",
      "Adenosine monophosphate             → adenosine monophosphate             → True\n",
      "Dapagliflozin                       → dapagliflozin                       → True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## map drug report name to actual drug name from dataset\n",
    "drug_data_clean[\"report_key\"] = drug_data_clean[\"Drug\"].map(_canon)\n",
    "drug_data_clean[\"report\"] = drug_data_clean[\"report_key\"].map(lambda k: reports_dict.get(k))\n",
    "print(\"---- DRUG → report_key → exists ----\")\n",
    "for d, k in drug_data_clean[[\"Drug\",\"report_key\"]].head(10).values:\n",
    "    print(f\"{d:<35} → {k:<35} → {k in reports_dict}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "178512f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Drug</th>\n",
       "      <th>Receptor</th>\n",
       "      <th>PDB_ID</th>\n",
       "      <th>report_key</th>\n",
       "      <th>report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Metformin</td>\n",
       "      <td>Acetyl-CoA carboxylase 2</td>\n",
       "      <td>3FF6,3TDC,2X24,3JRX,3JRW,2HJW,4HQ6,5KKN,3GLK,3...</td>\n",
       "      <td>metformin</td>\n",
       "      <td>In this retrospective study of locally advance...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cetuximab</td>\n",
       "      <td>Epidermal growth factor receptor</td>\n",
       "      <td>7SZ7,7SZ5,7SYE,7SYD,7SZ1,7SZ0,8HGS,8HGP,8HGO,5...</td>\n",
       "      <td>cetuximab</td>\n",
       "      <td>This case report links cetuximab (used as biol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bevacizumab</td>\n",
       "      <td>Vascular endothelial growth factor A</td>\n",
       "      <td>3V2A,5T89,8UWZ,6T9D,7KF1,7KF0,7KEZ,5FV2,5FV1,3...</td>\n",
       "      <td>bevacizumab</td>\n",
       "      <td>Bevacizumab was used in this metastatic CRC ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pioglitazone</td>\n",
       "      <td>Peroxisome proliferator-activated receptor gamma</td>\n",
       "      <td>3E00,3DZY,3DZU,7QB1,6L89,6K0T,6AD9,5HZC,5F9B,5...</td>\n",
       "      <td>pioglitazone</td>\n",
       "      <td>Pioglitazone, a PPARG (thiazolidinedione) agon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adenosine triphosphate</td>\n",
       "      <td>Tyrosine-protein kinase ABL1</td>\n",
       "      <td>5MO4,1OPK,1OPL,2FO0,8SSN,4XEY,6XR7,6XR6,2E2B,4...</td>\n",
       "      <td>adenosine triphosphate</td>\n",
       "      <td>The abstract notes tumor (including colorectal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Valproate</td>\n",
       "      <td>Glycogen synthase kinase-3 alpha</td>\n",
       "      <td>7SXF,7SXG,8VMG,8VMF,8VME,4NM7,4NM5,5K5N,6TCU,3...</td>\n",
       "      <td>valproate</td>\n",
       "      <td>The HDAC inhibitor sodium valproate reduced di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Venlafaxine</td>\n",
       "      <td>Serotonin transporter</td>\n",
       "      <td>6VRL,6VRK,6VRH,7TXT,7LWD,5I6Z,6W2C,6W2B,6DZZ,6...</td>\n",
       "      <td>venlafaxine</td>\n",
       "      <td>The abstract reports that venlafaxine provides...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Vildagliptin</td>\n",
       "      <td>Dipeptidyl peptidase 4</td>\n",
       "      <td>2QTB,2QT9,2BGR,2JID,3F8S,2QJR,3W2T,3VJM,3VJL,3...</td>\n",
       "      <td>vildagliptin</td>\n",
       "      <td>Vildagliptin, as a member of the DPP‑4 inhibit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Vitamin B6</td>\n",
       "      <td>Aromatic-L-amino-acid decarboxylase</td>\n",
       "      <td>9GNS,8ORA,8OR9,3RCH,3RBL,3RBF,9HRH,9HRI,1JS6,1JS3</td>\n",
       "      <td>vitamin b6</td>\n",
       "      <td>In this cohort of stage III colon cancer patie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Zanubrutinib</td>\n",
       "      <td>Tyrosine-protein kinase BTK</td>\n",
       "      <td>8GMB,4XI2,8FLG,6W07,6W06,6VXQ,6NZM,6HRT,6HRP,6...</td>\n",
       "      <td>zanubrutinib</td>\n",
       "      <td>The abstract lists zanubrutinib as one of seve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Drug                                          Receptor  \\\n",
       "0                Metformin                          Acetyl-CoA carboxylase 2   \n",
       "1                Cetuximab                  Epidermal growth factor receptor   \n",
       "2              Bevacizumab              Vascular endothelial growth factor A   \n",
       "3             Pioglitazone  Peroxisome proliferator-activated receptor gamma   \n",
       "4   Adenosine triphosphate                      Tyrosine-protein kinase ABL1   \n",
       "..                     ...                                               ...   \n",
       "63               Valproate                  Glycogen synthase kinase-3 alpha   \n",
       "64             Venlafaxine                             Serotonin transporter   \n",
       "65            Vildagliptin                            Dipeptidyl peptidase 4   \n",
       "66              Vitamin B6               Aromatic-L-amino-acid decarboxylase   \n",
       "67            Zanubrutinib                       Tyrosine-protein kinase BTK   \n",
       "\n",
       "                                               PDB_ID              report_key  \\\n",
       "0   3FF6,3TDC,2X24,3JRX,3JRW,2HJW,4HQ6,5KKN,3GLK,3...               metformin   \n",
       "1   7SZ7,7SZ5,7SYE,7SYD,7SZ1,7SZ0,8HGS,8HGP,8HGO,5...               cetuximab   \n",
       "2   3V2A,5T89,8UWZ,6T9D,7KF1,7KF0,7KEZ,5FV2,5FV1,3...             bevacizumab   \n",
       "3   3E00,3DZY,3DZU,7QB1,6L89,6K0T,6AD9,5HZC,5F9B,5...            pioglitazone   \n",
       "4   5MO4,1OPK,1OPL,2FO0,8SSN,4XEY,6XR7,6XR6,2E2B,4...  adenosine triphosphate   \n",
       "..                                                ...                     ...   \n",
       "63  7SXF,7SXG,8VMG,8VMF,8VME,4NM7,4NM5,5K5N,6TCU,3...               valproate   \n",
       "64  6VRL,6VRK,6VRH,7TXT,7LWD,5I6Z,6W2C,6W2B,6DZZ,6...             venlafaxine   \n",
       "65  2QTB,2QT9,2BGR,2JID,3F8S,2QJR,3W2T,3VJM,3VJL,3...            vildagliptin   \n",
       "66  9GNS,8ORA,8OR9,3RCH,3RBL,3RBF,9HRH,9HRI,1JS6,1JS3              vitamin b6   \n",
       "67  8GMB,4XI2,8FLG,6W07,6W06,6VXQ,6NZM,6HRT,6HRP,6...            zanubrutinib   \n",
       "\n",
       "                                               report  \n",
       "0   In this retrospective study of locally advance...  \n",
       "1   This case report links cetuximab (used as biol...  \n",
       "2   Bevacizumab was used in this metastatic CRC ca...  \n",
       "3   Pioglitazone, a PPARG (thiazolidinedione) agon...  \n",
       "4   The abstract notes tumor (including colorectal...  \n",
       "..                                                ...  \n",
       "63  The HDAC inhibitor sodium valproate reduced di...  \n",
       "64  The abstract reports that venlafaxine provides...  \n",
       "65  Vildagliptin, as a member of the DPP‑4 inhibit...  \n",
       "66  In this cohort of stage III colon cancer patie...  \n",
       "67  The abstract lists zanubrutinib as one of seve...  \n",
       "\n",
       "[68 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "reports_dict = load_reports_dir(summaries_path)\n",
    "\n",
    "pat_drop_pmid = re.compile(r\"^PMID:.*$\", re.I | re.M)\n",
    "drug_data_clean[\"report\"] = drug_data_clean[\"report_key\"].map(reports_dict.get)\n",
    "\n",
    "\n",
    "drug_data_clean[\"report\"] = drug_data_clean[\"report\"].apply(\n",
    "    lambda txt: pat_drop_pmid.sub(\"\", txt).strip() if isinstance(txt, str) else txt\n",
    ")\n",
    "drug_data_clean = drug_data_clean.dropna(subset=[\"report\"]).reset_index(drop=True)\n",
    "print(drug_data_clean.shape)\n",
    "\n",
    "drug_data_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00aad130",
   "metadata": {},
   "source": [
    "### read train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "906e28e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 14\n"
     ]
    }
   ],
   "source": [
    "train_df_path = file_paths[\"train_val_paths\"][\"train_path\"]\n",
    "val_df_path = file_paths[\"train_val_paths\"][\"val_path\"]\n",
    "\n",
    "train_df = pd.read_csv(train_df_path)\n",
    "val_df = pd.read_csv(val_df_path)\n",
    "\n",
    "print(len(train_df), len(val_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607ae33e",
   "metadata": {},
   "source": [
    "## llm training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6b661e9-473a-4a3b-82a6-4253590dd43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The key to life is to never give up.\\nThis was the message that the National Association of Realtors (NAR) chose to focus on in its latest radio spot. The spot is one of a series of spots that NAR has run in conjunction with a new consumer awareness campaign, “Buy with Confidence.”\\nIn the spot, a Realtor says, “The key to life is to never give up. That’s what I tell my clients. Never give up. I tell my clients, ‘The key to life is to never give up.’” The Realtor then goes on to explain why it’s important to never give up.\\n“If you’re ready to buy a home, there are a lot of things that can get in your way. If you’re ready to buy a home, there are a lot of things that can get in your way. We’re going to help you navigate the path to your new home.”\\nWhile the spot is about the importance of never giving up, it’s also a reminder that the key to life is to always be prepared to face the challenges that come your way.\\nThe key to life is to never give up. That’s what I tell my clients. I tell my clients, ‘The key to life is to never give up.’\\nThe key to'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model_id, \n",
    "    dtype=torch.bfloat16, \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "pipe(\"The key to life is\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2db29f1-6150-4d34-84f3-91d347dbff26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0},          # <-- force all modules to GPU:0\n",
    "    dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "print(\"Loaded on:\", next(model.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369dab85",
   "metadata": {},
   "source": [
    "### load model configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab3a2ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID   = PARAMS[\"model\"][\"id\"]\n",
    "MODEL_OUTPUT_DIR = PARAMS[\"model\"][\"output_dir\"] ## save model configs here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ee3e5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _dtype_from_str(s: str):\n",
    "    s = (s or \"\").lower()\n",
    "    if s in (\"bf16\", \"bfloat16\"):\n",
    "        return torch.bfloat16\n",
    "    if s in (\"fp16\", \"float16\", \"half\"):\n",
    "        return torch.float16\n",
    "    if s in (\"fp32\", \"float32\"):\n",
    "        return torch.float32\n",
    "    if s in (\"auto\", \"\"):\n",
    "        return None\n",
    "    raise ValueError(f\"Unknown dtype string: {s!r}\")\n",
    "\n",
    "\n",
    "def _normalize_max_memory(mm: Optional[Dict[Any, str]]) -> Optional[Dict[Any, str]]:\n",
    "    \"\"\"\n",
    "    - Convert string keys like \"0\" to int 0.\n",
    "    - Drop GPU entries if there is no CUDA device.\n",
    "    - Leave 'cpu' / 'mps' / 'disk' untouched.\n",
    "    \"\"\"\n",
    "    if not mm:\n",
    "        return None\n",
    "\n",
    "    has_cuda = torch.cuda.is_available() and torch.cuda.device_count() > 0\n",
    "    out: Dict[Any, str] = {}\n",
    "\n",
    "    for k, v in mm.items():\n",
    "        # \"0\" -> 0\n",
    "        if isinstance(k, str) and k.isdigit():\n",
    "            k_int = int(k)\n",
    "            if has_cuda and k_int < torch.cuda.device_count():\n",
    "                out[k_int] = v\n",
    "            # if no CUDA or out-of-range GPU id, just skip\n",
    "        else:\n",
    "            out[k] = v\n",
    "\n",
    "    return out or None\n",
    "    \n",
    "def configure_precision_flags(params: Dict[str, Any]) -> None:\n",
    "    has_cuda = torch.cuda.is_available()\n",
    "    supports_bf16 = has_cuda and torch.cuda.is_bf16_supported()\n",
    "\n",
    "    # Model / quantization dtype (for from_pretrained)\n",
    "    if supports_bf16:\n",
    "        params[\"model\"][\"dtype\"] = \"bf16\"\n",
    "        params[\"bnb\"][\"compute_dtype\"] = \"bf16\"\n",
    "    elif has_cuda:\n",
    "        params[\"model\"][\"dtype\"] = \"fp16\"\n",
    "        params[\"bnb\"][\"compute_dtype\"] = \"fp16\"\n",
    "    else:\n",
    "        # pure CPU: keep it fp32\n",
    "        params[\"model\"][\"dtype\"] = \"fp32\"\n",
    "        params[\"bnb\"][\"compute_dtype\"] = \"fp32\"\n",
    "\n",
    "    # TrainingArguments precision flags – MUST be mutually exclusive\n",
    "    params[\"train\"][\"bf16\"] = bool(supports_bf16)\n",
    "    params[\"train\"][\"fp16\"] = bool(has_cuda and not supports_bf16)\n",
    "\n",
    "\n",
    "def load_dataframe(path: str):\n",
    "    df = pd.read_csv(path)\n",
    "    drop_cols = PARAMS[\"data\"][\"dropna_cols\"]\n",
    "    df = df.dropna(subset=drop_cols).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "class SafeDict(dict):\n",
    "    def __missing__(self, key):\n",
    "        return \"\"  # default blank for any missing placeholder\n",
    "\n",
    "def get_first_key(ex, keys):\n",
    "    \"\"\"Return first non-empty value among candidate keys from a row/example.\"\"\"\n",
    "    for k in keys:\n",
    "        if k in ex and ex[k] is not None and str(ex[k]).strip() != \"\":\n",
    "            return str(ex[k])\n",
    "    return \"\"\n",
    "\n",
    "from typing import Any, Dict, List, Optional\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Optional fallback template (only used if tokenizer.chat_template is missing)\n",
    "LLAMA3_FALLBACK_CHAT_TEMPLATE = \"\"\"\\\n",
    "<|begin_of_text|>\n",
    "{%- for message in messages -%}\n",
    "{%- if message['role'] == 'system' -%}\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "{{ message['content'] }}<|eot_id|>\n",
    "{%- elif message['role'] == 'user' -%}\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "{{ message['content'] }}<|eot_id|>\n",
    "{%- elif message['role'] == 'assistant' -%}\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "{{ message['content'] }}<|eot_id|>\n",
    "{%- endif -%}\n",
    "{%- endfor -%}\n",
    "{%- if add_generation_prompt -%}\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "{%- endif -%}\n",
    "\"\"\"\n",
    "\n",
    "def _get_first_nonempty(ex: Dict[str, Any], keys: List[str]) -> str:\n",
    "    for k in keys:\n",
    "        v = ex.get(k, None)\n",
    "        if v is None:\n",
    "            continue\n",
    "        s = str(v).strip()\n",
    "        if s != \"\":\n",
    "            return s\n",
    "    return \"\"\n",
    "\n",
    "def chat_format_map_fn(\n",
    "    ex: Dict[str, Any],\n",
    "    training_prompt: str,\n",
    "    system_prompt: str,\n",
    "    few_shots: List[Dict[str, str]],\n",
    "    tokenizer: AutoTokenizer,\n",
    "    # If you have a target column in your CSV, list its possible names here:\n",
    "    target_keys: Optional[List[str]] = None,\n",
    ") -> Dict[str, str]:\n",
    "    vals = {\n",
    "        \"Drug\": ex.get(\"Drug\", \"\"),\n",
    "        \"Receptor\": ex.get(\"Receptor\", \"\"),\n",
    "        \"report\": ex.get(\"report\", \"\"),\n",
    "        \"PDB_ID\": ex.get(\"PDB_ID\", \"\"),\n",
    "    }\n",
    "    user_text = training_prompt.format_map(SafeDict(vals))\n",
    "\n",
    "    msgs: List[Dict[str, str]] = []\n",
    "    if system_prompt and str(system_prompt).strip():\n",
    "        msgs.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "\n",
    "    for shot in few_shots or []:\n",
    "        if shot and \"user\" in shot and \"assistant\" in shot:\n",
    "            msgs.append({\"role\": \"user\", \"content\": shot[\"user\"]})\n",
    "            msgs.append({\"role\": \"assistant\", \"content\": shot[\"assistant\"]})\n",
    "\n",
    "    msgs.append({\"role\": \"user\", \"content\": user_text})\n",
    "\n",
    "    # --- Supervised target (HIGHLY recommended for SFT) ---\n",
    "    # Put your ground-truth answer in the CSV (e.g., \"answer\" or \"output\")\n",
    "    target_keys = target_keys or [\"answer\", \"output\", \"response\", \"assistant\", \"completion\", \"label\", \"gold\"]\n",
    "    target_text = _get_first_nonempty(ex, target_keys)\n",
    "\n",
    "    if target_text:\n",
    "        # We include the assistant message, so we should NOT add a generation prompt.\n",
    "        msgs.append({\"role\": \"assistant\", \"content\": target_text})\n",
    "        add_gen = False\n",
    "    else:\n",
    "        # No ground-truth target column found: keep gen prompt so samples end at assistant header\n",
    "        add_gen = True\n",
    "\n",
    "    chat_template = tokenizer.chat_template or LLAMA3_FALLBACK_CHAT_TEMPLATE\n",
    "\n",
    "    rendered = tokenizer.apply_chat_template(\n",
    "        msgs,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=add_gen,\n",
    "        chat_template=chat_template,  # safe even when tokenizer.chat_template exists\n",
    "    )\n",
    "    return {\"text\": rendered}\n",
    "\n",
    "def plain_format_map_fn(\n",
    "    ex: Dict[str, Any],\n",
    "    training_prompt: str,\n",
    ") -> Dict[str, str]:\n",
    "    vals = {\n",
    "        \"Drug\": ex.get(\"Drug\", \"\"),\n",
    "        \"Receptor\": ex.get(\"Receptor\", \"\"),\n",
    "        \"report\": ex.get(\"report\", \"\"),\n",
    "        \"PDB_ID\": ex.get(\"PDB_ID\", \"\"),\n",
    "    }\n",
    "    text = training_prompt.format_map(SafeDict(vals))\n",
    "    return {\"text\": text}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_dataset(\n",
    "    csv_path: str,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    params: Dict[str, Any],\n",
    "    prompts: Dict[str, Any],\n",
    ") -> Dataset:\n",
    "    df = load_dataframe(csv_path)\n",
    "\n",
    "    p = prompts[\"prompts\"]\n",
    "    training_prompt = p[\"train_user_template\"]\n",
    "    system_prompt   = p[\"system\"]\n",
    "    use_chat_format = p.get(\"use_chat_format\", False)\n",
    "    few_shots       = p.get(\"few_shots\", [])\n",
    "\n",
    "    cols = list(df.columns)\n",
    "    ds = Dataset.from_pandas(df, preserve_index=False)\n",
    "\n",
    "    if use_chat_format:\n",
    "        # HuggingFace Datasets map passes only the example dict, so we wrap\n",
    "        # our extra arguments using a closure-like helper via kwargs.\n",
    "        def _wrapped_chat_map_fn(ex: Dict[str, Any]) -> Dict[str, str]:\n",
    "            return chat_format_map_fn(\n",
    "                ex=ex,\n",
    "                training_prompt=training_prompt,\n",
    "                system_prompt=system_prompt,\n",
    "                few_shots=few_shots,\n",
    "                tokenizer=tokenizer,\n",
    "            )\n",
    "\n",
    "        ds = ds.map(_wrapped_chat_map_fn, remove_columns=cols)\n",
    "    else:\n",
    "        def _wrapped_plain_map_fn(ex: Dict[str, Any]) -> Dict[str, str]:\n",
    "            return plain_format_map_fn(\n",
    "                ex=ex,\n",
    "                training_prompt=training_prompt,\n",
    "            )\n",
    "\n",
    "        ds = ds.map(_wrapped_plain_map_fn, remove_columns=cols)\n",
    "\n",
    "    ds = ds.train_test_split(\n",
    "        test_size=params[\"data\"][\"test_size\"],\n",
    "        seed=params[\"data\"][\"seed\"],\n",
    "    )\n",
    "    return ds\n",
    "\n",
    "\n",
    "\n",
    "def get_model_and_tokenizer(params):\n",
    "    model_id = params[\"model\"][\"id\"]\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_id,\n",
    "        use_fast=True,\n",
    "        trust_remote_code=params[\"model\"].get(\"trust_remote_code\", True),\n",
    "    )\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # bitsandbytes / QLoRA config\n",
    "    bnb_cfg = None\n",
    "    if params[\"model\"].get(\"load_in_4bit\", False):\n",
    "        bnb_cfg = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=params[\"bnb\"][\"quant_type\"],\n",
    "            bnb_4bit_use_double_quant=params[\"bnb\"][\"double_quant\"],\n",
    "            bnb_4bit_compute_dtype=_dtype_from_str(params[\"bnb\"][\"compute_dtype\"]) or torch.float16,\n",
    "        )\n",
    "\n",
    "    # normalize max_memory from YAML\n",
    "    max_memory = _normalize_max_memory(params[\"model\"].get(\"max_memory\"))\n",
    "\n",
    "    # use dtype (new API) instead of torch_dtype\n",
    "    dtype = _dtype_from_str(params[\"model\"].get(\"dtype\"))\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=bnb_cfg,\n",
    "        device_map=\"auto\",\n",
    "        dtype=dtype,                           \n",
    "        max_memory=max_memory,\n",
    "        offload_folder=params[\"model\"][\"offload_folder\"],\n",
    "        low_cpu_mem_usage=True,\n",
    "        trust_remote_code=params[\"model\"][\"trust_remote_code\"],\n",
    "    )\n",
    "\n",
    "    model.config.use_cache = False\n",
    "    model.gradient_checkpointing_enable()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_lora_wrapped(model, params: Dict[str, Any]):\n",
    "    lconf = params[\"lora\"]\n",
    "    lora_cfg = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        r=lconf[\"r\"],\n",
    "        lora_alpha=lconf[\"alpha\"],\n",
    "        lora_dropout=lconf[\"dropout\"],\n",
    "        target_modules=lconf[\"target_modules\"],\n",
    "        bias=lconf[\"bias\"],\n",
    "    )\n",
    "    model = get_peft_model(model, lora_cfg)\n",
    "    return model\n",
    "\n",
    "\n",
    "def main(\n",
    "    train_csv: str,\n",
    "    file_paths: Dict[str, Any],\n",
    "    PARAMS: Dict[str, Any],\n",
    "    prompts: Dict[str, Any],\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Run SFT training.\n",
    "\n",
    "    Args:\n",
    "        train_csv: Path to the training CSV.\n",
    "        file_paths: Config dict from P3-config.yaml (if you need more paths).\n",
    "        params: Training hyperparameters and model settings.\n",
    "        prompts: Prompt templates and chat-format toggles.\n",
    "    \"\"\"\n",
    "    model, tokenizer = get_model_and_tokenizer(PARAMS)\n",
    "    model = get_lora_wrapped(model, PARAMS)\n",
    "\n",
    "    ds = build_dataset(train_csv, tokenizer, PARAMS, prompts)\n",
    "\n",
    "\n",
    "    max_len = PARAMS[\"data\"][\"max_seq_length\"]\n",
    "\n",
    "    training_args = SFTConfig(\n",
    "        output_dir=PARAMS[\"model\"][\"output_dir\"],\n",
    "        num_train_epochs=PARAMS[\"train\"][\"epochs\"],\n",
    "        per_device_train_batch_size=PARAMS[\"train\"][\"per_device_train_batch_size\"],\n",
    "        per_device_eval_batch_size=PARAMS[\"train\"][\"per_device_eval_batch_size\"],\n",
    "        gradient_accumulation_steps=PARAMS[\"train\"][\"gradient_accumulation_steps\"],\n",
    "        learning_rate=float(PARAMS[\"train\"][\"learning_rate\"]),\n",
    "        lr_scheduler_type=PARAMS[\"train\"][\"scheduler\"],\n",
    "        warmup_ratio=float(PARAMS[\"train\"][\"warmup_ratio\"]),\n",
    "        weight_decay=float(PARAMS[\"train\"][\"weight_decay\"]),\n",
    "        logging_steps=PARAMS[\"train\"][\"logging_steps\"],\n",
    "        # eval_strategy=PARAMS[\"train\"][\"evaluation_strategy\"],   # <- NEW (replaces evaluation_strategy)\n",
    "        # eval_steps=PARAMS[\"train\"][\"eval_steps\"],\n",
    "        save_steps=PARAMS[\"train\"][\"save_steps\"],\n",
    "        save_total_limit=PARAMS[\"train\"][\"save_total_limit\"],\n",
    "        bf16=PARAMS[\"train\"][\"bf16\"],\n",
    "        fp16=PARAMS[\"train\"][\"fp16\"],\n",
    "        gradient_checkpointing=PARAMS[\"train\"][\"gradient_checkpointing\"],\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": False}, # <- NEW to silence Torch 2.5 warning\n",
    "        packing=PARAMS[\"data\"][\"packing\"],\n",
    "        optim=PARAMS[\"train\"][\"optim\"],\n",
    "        max_grad_norm=PARAMS[\"train\"][\"max_grad_norm\"],\n",
    "        seed=PARAMS[\"train\"][\"seed\"],\n",
    "        report_to=PARAMS[\"train\"][\"report_to\"],\n",
    "        load_best_model_at_end=PARAMS[\"train\"][\"load_best_model\"],\n",
    "    \n",
    "        # Move these here (don’t pass them to SFTTrainer):\n",
    "        # max_seq_length=PARAMS[\"data\"][\"max_seq_length\"],\n",
    "        dataset_text_field=\"text\",\n",
    "    )\n",
    "\n",
    "        \n",
    "    # early_stopping = EarlyStoppingCallback(\n",
    "    #     early_stopping_patience=1,      # VERY small because only 3 epochs\n",
    "    #     early_stopping_threshold=0.0\n",
    "    # )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,\n",
    "        train_dataset=ds[\"train\"],\n",
    "        args=training_args,         # <- contains max_seq_length + dataset_text_field\n",
    "        # callbacks=[early_stopping],\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model()\n",
    "    tokenizer.save_pretrained(PARAMS[\"model\"][\"output_dir\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d75ab7af-6d10-4dfc-bddd-bc4d028a450f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "030d974a18af40acbc5aab2961b7fb84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/54 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d89024125037426cb14f29edf9ae05c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/43 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7518ddf19de34d449bdbe3205b2eddf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/43 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad7215c525e24f0ab49df1a27bb8c2a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/43 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:17, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load configs from YAML\n",
    "    file_paths = load_config(yaml_path=\"P3-config.yaml\")\n",
    "    PARAMS = load_config(yaml_path=\"training_params.yaml\")\n",
    "    prompts = load_config(yaml_path=\"prompts.yaml\")\n",
    "\n",
    "    configure_precision_flags(PARAMS)\n",
    "\n",
    "\n",
    "    # Default to train_path from config, but you can pass any CSV you want\n",
    "    train_csv_path = file_paths[\"train_val_paths\"][\"train_path\"]\n",
    "\n",
    "    main(\n",
    "        train_csv=train_csv_path,\n",
    "        file_paths=file_paths,\n",
    "        PARAMS=PARAMS,\n",
    "        prompts=prompts,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0799a8a-fd2e-40d2-849f-4a3cff78e049",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fypkernel2",
   "language": "python",
   "name": "fyp_kernel2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
