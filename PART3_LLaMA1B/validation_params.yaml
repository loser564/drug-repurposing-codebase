# ================================
# Validation / Inference Parameters
# ================================

base_model_id: "meta-llama/Llama-3.2-1B-Instruct"
adapter_dir: "llama-1b-qlora-finetune"


save_paths:
  inference_reports: "llama_1b_inference_reports.csv"
  real_world_test_basic: "llama_1b_basic_realworldtest.csv"
  real_world_test_onco: "llama_1b_onco_realworldtest.csv"
  real_world_test_review: "llama_1b_review_realworldtest.csv"
  default: "unspecified_llama_1b.csv"

merge_16bit: false

# -------------------------------
# Quantization (for 4-bit inference)
# -------------------------------
quantization:
  load_in_4bit: true
  quant_type: "nf4"       # "nf4" | "fp4"
  double_quant: true
  compute_dtype: "auto"   # computed in Python: "bf16" or "fp16"


# -------------------------------
# Generation parameters
# -------------------------------
generation:
  temperature: 0.2
  top_p: 0.9
  top_k: 50
  max_new_tokens: 4096 ## max of llama-1b
  do_sample: true
  repetition_penalty: 1.05
  stop_strings: []           # optional custom stop tokens
  # beam search fields (optional)
  # num_beams: 5
  # num_return_sequences: 1

# -------------------------------
# Batch / Device
# -------------------------------
batch:
  batch_size: 1
  device_map: "auto"

# -------------------------------
# Expected column names from validation/test CSV
# -------------------------------
columns:
  drug: "Drug"
  receptor: "Receptor"
  pdb: "PDB_ID"    # optional
