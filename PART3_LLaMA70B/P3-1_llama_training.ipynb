{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf20fdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Standard Library\n",
    "# ================================\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "# ================================\n",
    "# Third-Party Libraries\n",
    "# ================================\n",
    "import bitsandbytes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import yaml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "# HuggingFace Transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback,\n",
    "    GenerationConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    __version__ as HF_VER,\n",
    ")\n",
    "\n",
    "# PEFT (LoRA / QLoRA)\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    TaskType,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "\n",
    "# TRL (Supervised Fine-Tuning Trainer)\n",
    "from trl import SFTConfig, SFTTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34ec2cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(yaml_path):\n",
    "    with open(yaml_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "    \n",
    "file_paths = load_config(yaml_path=\"P3-config.yaml\")\n",
    "PARAMS = load_config(yaml_path=\"training_params.yaml\")\n",
    "prompts = load_config(yaml_path=\"prompts.yaml\")\n",
    "\n",
    "dtype = \"bf16\" if torch.cuda.is_bf16_supported() else \"fp16\"\n",
    "PARAMS[\"model\"][\"dtype\"] = dtype\n",
    "PARAMS[\"bnb\"][\"compute_dtype\"] = dtype\n",
    "PARAMS[\"train\"][\"bf16\"] = torch.cuda.is_bf16_supported()\n",
    "PARAMS[\"train\"][\"fp16\"] = not torch.cuda.is_bf16_supported()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e1931fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Drug</th>\n",
       "      <th>Receptor</th>\n",
       "      <th>PDB_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Metformin</td>\n",
       "      <td>Acetyl-CoA carboxylase 2</td>\n",
       "      <td>3FF6,3TDC,2X24,3JRX,3JRW,2HJW,4HQ6,5KKN,3GLK,3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cetuximab</td>\n",
       "      <td>Epidermal growth factor receptor</td>\n",
       "      <td>7SZ7,7SZ5,7SYE,7SYD,7SZ1,7SZ0,8HGS,8HGP,8HGO,5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bevacizumab</td>\n",
       "      <td>Vascular endothelial growth factor A</td>\n",
       "      <td>3V2A,5T89,8UWZ,6T9D,7KF1,7KF0,7KEZ,5FV2,5FV1,3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pioglitazone</td>\n",
       "      <td>Peroxisome proliferator-activated receptor gamma</td>\n",
       "      <td>3E00,3DZY,3DZU,7QB1,6L89,6K0T,6AD9,5HZC,5F9B,5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adenosine triphosphate</td>\n",
       "      <td>Tyrosine-protein kinase ABL1</td>\n",
       "      <td>5MO4,1OPK,1OPL,2FO0,8SSN,4XEY,6XR7,6XR6,2E2B,4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Drug                                          Receptor  \\\n",
       "0               Metformin                          Acetyl-CoA carboxylase 2   \n",
       "1               Cetuximab                  Epidermal growth factor receptor   \n",
       "2             Bevacizumab              Vascular endothelial growth factor A   \n",
       "3            Pioglitazone  Peroxisome proliferator-activated receptor gamma   \n",
       "4  Adenosine triphosphate                      Tyrosine-protein kinase ABL1   \n",
       "\n",
       "                                              PDB_ID  \n",
       "0  3FF6,3TDC,2X24,3JRX,3JRW,2HJW,4HQ6,5KKN,3GLK,3...  \n",
       "1  7SZ7,7SZ5,7SYE,7SYD,7SZ1,7SZ0,8HGS,8HGP,8HGO,5...  \n",
       "2  3V2A,5T89,8UWZ,6T9D,7KF1,7KF0,7KEZ,5FV2,5FV1,3...  \n",
       "3  3E00,3DZY,3DZU,7QB1,6L89,6K0T,6AD9,5HZC,5F9B,5...  \n",
       "4  5MO4,1OPK,1OPL,2FO0,8SSN,4XEY,6XR7,6XR6,2E2B,4...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "drug_data_path = file_paths[\"input_file_paths\"][\"drug_data_path\"]\n",
    "\n",
    "drug_data_df = pd.read_csv(drug_data_path)\n",
    "drug_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebb694ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "summaries_path = file_paths[\"input_file_paths\"][\"summaries_path\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bfab04",
   "metadata": {},
   "source": [
    "## Data cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b9c563f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Drug</th>\n",
       "      <th>Receptor</th>\n",
       "      <th>PDB_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Metformin</td>\n",
       "      <td>Acetyl-CoA carboxylase 2</td>\n",
       "      <td>3FF6,3TDC,2X24,3JRX,3JRW,2HJW,4HQ6,5KKN,3GLK,3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cetuximab</td>\n",
       "      <td>Epidermal growth factor receptor</td>\n",
       "      <td>7SZ7,7SZ5,7SYE,7SYD,7SZ1,7SZ0,8HGS,8HGP,8HGO,5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bevacizumab</td>\n",
       "      <td>Vascular endothelial growth factor A</td>\n",
       "      <td>3V2A,5T89,8UWZ,6T9D,7KF1,7KF0,7KEZ,5FV2,5FV1,3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pioglitazone</td>\n",
       "      <td>Peroxisome proliferator-activated receptor gamma</td>\n",
       "      <td>3E00,3DZY,3DZU,7QB1,6L89,6K0T,6AD9,5HZC,5F9B,5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adenosine triphosphate</td>\n",
       "      <td>Tyrosine-protein kinase ABL1</td>\n",
       "      <td>5MO4,1OPK,1OPL,2FO0,8SSN,4XEY,6XR7,6XR6,2E2B,4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Drug                                          Receptor  \\\n",
       "0               Metformin                          Acetyl-CoA carboxylase 2   \n",
       "1               Cetuximab                  Epidermal growth factor receptor   \n",
       "2             Bevacizumab              Vascular endothelial growth factor A   \n",
       "3            Pioglitazone  Peroxisome proliferator-activated receptor gamma   \n",
       "4  Adenosine triphosphate                      Tyrosine-protein kinase ABL1   \n",
       "\n",
       "                                              PDB_ID  \n",
       "0  3FF6,3TDC,2X24,3JRX,3JRW,2HJW,4HQ6,5KKN,3GLK,3...  \n",
       "1  7SZ7,7SZ5,7SYE,7SYD,7SZ1,7SZ0,8HGS,8HGP,8HGO,5...  \n",
       "2  3V2A,5T89,8UWZ,6T9D,7KF1,7KF0,7KEZ,5FV2,5FV1,3...  \n",
       "3  3E00,3DZY,3DZU,7QB1,6L89,6K0T,6AD9,5HZC,5F9B,5...  \n",
       "4  5MO4,1OPK,1OPL,2FO0,8SSN,4XEY,6XR7,6XR6,2E2B,4...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_data_clean = drug_data_df.copy()\n",
    "drug_data_clean.columns\n",
    "\n",
    "drug_data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49e1f672",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _norm(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", str(s).strip().upper())\n",
    "\n",
    "def _safe(s: Optional[str]) -> str:\n",
    "    return \"\" if s is None else str(s).strip()\n",
    "\n",
    "def _read_text_file(path: str) -> Optional[str]:\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            txt = f.read().strip()\n",
    "            return txt if txt else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _canon(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Lowercase; turn any run of non-alphanumerics (incl. underscores, hyphens, commas)\n",
    "    into a single space; then collapse spaces.\n",
    "    \"\"\"\n",
    "    s = str(s).lower().strip()\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \" \", s)   # underscores, hyphens, slashes -> space\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "\n",
    "def load_reports_dir(reports_dir: str) -> Dict[str, str]:\n",
    "    pat_file = re.compile(r\"^(.+?)_summary\\.txt$\", re.I)\n",
    "    pat_rm   = re.compile(r\"PMID:\\s*\\d+\\s*(?:\\n\\s*)?(?:no relevant information found\\.?)\",\n",
    "                          re.I)\n",
    "\n",
    "    out = {}\n",
    "\n",
    "    for p in glob.glob(os.path.join(reports_dir, \"*_summary.txt\")):\n",
    "        fn = os.path.basename(p)\n",
    "        m = pat_file.match(fn)\n",
    "        if not m:\n",
    "            continue\n",
    "\n",
    "        key = _canon(m.group(1))\n",
    "\n",
    "\n",
    "        txt = _read_text_file(p)\n",
    "        if not txt:\n",
    "            continue\n",
    "\n",
    "        # remove junk pmid blocks\n",
    "        txt = pat_rm.sub(\"\", txt)\n",
    "\n",
    "        # collapse lines\n",
    "        txt = re.sub(r\"\\n{3,}\", \"\\n\\n\", txt).strip()\n",
    "\n",
    "        # *** reject empty after cleaning ***\n",
    "        if not txt:\n",
    "            # optional debug: print(f\"SKIP blank: {fn}\")\n",
    "            continue\n",
    "\n",
    "        out[key] = txt\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b02f4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 | amphetamine\n",
      "PMID: 39287256\n",
      "The study found KEGG enrichment of shared NASH/type 2 diabetes targets in the \"amphetamine\n",
      "addiction\" pathway alongside colorectal cancer, PPAR signaling and tollâ€‘like receptor signalin ...\n",
      "\n",
      "01 | semaglutide\n",
      "PMID: 40437949\n",
      "Semaglutide, as a GLPâ€‘1 receptor agonist used to treat diabetes and obesity, was part of trials\n",
      "pooled here showing no overall cancer risk but a small increased colorectal cancer signal ...\n",
      "\n",
      "02 | adenosine monophosphate\n",
      "PMID: 37071615\n",
      "Patchouli alcohol (PA) treatment increased phosphorylation (activation) of 5' adenosine\n",
      "monophosphateâ€‘activated protein kinase (AMPK) alongside protein kinase B (Akt) in differentiated\n",
      " ...\n",
      "\n",
      "03 | hydroxychloroquine\n",
      "PMID: 33608672\n",
      "The study shows that blocking autophagy with chloroquine potentiated killing of KRASâ€‘mutant CRC\n",
      "cells treated with glycolysis and OXPHOS inhibitors; by extension, hydroxychloroquine (a  ...\n",
      "\n",
      "04 | cimetidine\n",
      "PMID: 12938277\n",
      "In this case report of a diabetic patient with ascending colon carcinoma and lung metastases,\n",
      "postoperative oral cimetidine (800 mg/day) given with 5'-DFUR was associated with relativel ...\n",
      "\n",
      "05 | thiazolidinedione\n",
      "PMID: 31887708\n",
      "In this large population-based cohort of patients with newly diagnosed type 2 diabetes, use of\n",
      "thiazolidinediones was not associated with a change in colorectal cancer risk. The study t ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reports_dict = load_reports_dir(summaries_path)\n",
    "\n",
    "for i, (drug, text) in enumerate(reports_dict.items()):\n",
    "    print(f\"{i:02d} | {drug}\")\n",
    "    print(text[:200], \"...\\n\")      # first 200 chars preview\n",
    "\n",
    "    if i> 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59afbaa6",
   "metadata": {},
   "source": [
    "## merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53c7dd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- DRUG â†’ report_key â†’ exists ----\n",
      "Metformin                           â†’ metformin                           â†’ True\n",
      "Cetuximab                           â†’ cetuximab                           â†’ True\n",
      "Bevacizumab                         â†’ bevacizumab                         â†’ True\n",
      "Pioglitazone                        â†’ pioglitazone                        â†’ True\n",
      "Adenosine triphosphate              â†’ adenosine triphosphate              â†’ True\n",
      "Thiazolidinedione                   â†’ thiazolidinedione                   â†’ True\n",
      "Estrogen                            â†’ estrogen                            â†’ True\n",
      "Exenatide                           â†’ exenatide                           â†’ True\n",
      "Adenosine monophosphate             â†’ adenosine monophosphate             â†’ True\n",
      "Dapagliflozin                       â†’ dapagliflozin                       â†’ True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## map drug report name to actual drug name from dataset\n",
    "drug_data_clean[\"report_key\"] = drug_data_clean[\"Drug\"].map(_canon)\n",
    "drug_data_clean[\"report\"] = drug_data_clean[\"report_key\"].map(lambda k: reports_dict.get(k))\n",
    "print(\"---- DRUG â†’ report_key â†’ exists ----\")\n",
    "for d, k in drug_data_clean[[\"Drug\",\"report_key\"]].head(10).values:\n",
    "    print(f\"{d:<35} â†’ {k:<35} â†’ {k in reports_dict}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "178512f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Drug</th>\n",
       "      <th>Receptor</th>\n",
       "      <th>PDB_ID</th>\n",
       "      <th>report_key</th>\n",
       "      <th>report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Metformin</td>\n",
       "      <td>Acetyl-CoA carboxylase 2</td>\n",
       "      <td>3FF6,3TDC,2X24,3JRX,3JRW,2HJW,4HQ6,5KKN,3GLK,3...</td>\n",
       "      <td>metformin</td>\n",
       "      <td>In this retrospective study of locally advance...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cetuximab</td>\n",
       "      <td>Epidermal growth factor receptor</td>\n",
       "      <td>7SZ7,7SZ5,7SYE,7SYD,7SZ1,7SZ0,8HGS,8HGP,8HGO,5...</td>\n",
       "      <td>cetuximab</td>\n",
       "      <td>This case report links cetuximab (used as biol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bevacizumab</td>\n",
       "      <td>Vascular endothelial growth factor A</td>\n",
       "      <td>3V2A,5T89,8UWZ,6T9D,7KF1,7KF0,7KEZ,5FV2,5FV1,3...</td>\n",
       "      <td>bevacizumab</td>\n",
       "      <td>Bevacizumab was used in this metastatic CRC ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pioglitazone</td>\n",
       "      <td>Peroxisome proliferator-activated receptor gamma</td>\n",
       "      <td>3E00,3DZY,3DZU,7QB1,6L89,6K0T,6AD9,5HZC,5F9B,5...</td>\n",
       "      <td>pioglitazone</td>\n",
       "      <td>Pioglitazone, a PPARG (thiazolidinedione) agon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adenosine triphosphate</td>\n",
       "      <td>Tyrosine-protein kinase ABL1</td>\n",
       "      <td>5MO4,1OPK,1OPL,2FO0,8SSN,4XEY,6XR7,6XR6,2E2B,4...</td>\n",
       "      <td>adenosine triphosphate</td>\n",
       "      <td>The abstract notes tumor (including colorectal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Valproate</td>\n",
       "      <td>Glycogen synthase kinase-3 alpha</td>\n",
       "      <td>7SXF,7SXG,8VMG,8VMF,8VME,4NM7,4NM5,5K5N,6TCU,3...</td>\n",
       "      <td>valproate</td>\n",
       "      <td>The HDAC inhibitor sodium valproate reduced di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Venlafaxine</td>\n",
       "      <td>Serotonin transporter</td>\n",
       "      <td>6VRL,6VRK,6VRH,7TXT,7LWD,5I6Z,6W2C,6W2B,6DZZ,6...</td>\n",
       "      <td>venlafaxine</td>\n",
       "      <td>The abstract reports that venlafaxine provides...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Vildagliptin</td>\n",
       "      <td>Dipeptidyl peptidase 4</td>\n",
       "      <td>2QTB,2QT9,2BGR,2JID,3F8S,2QJR,3W2T,3VJM,3VJL,3...</td>\n",
       "      <td>vildagliptin</td>\n",
       "      <td>Vildagliptin, as a member of the DPPâ€‘4 inhibit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Vitamin B6</td>\n",
       "      <td>Aromatic-L-amino-acid decarboxylase</td>\n",
       "      <td>9GNS,8ORA,8OR9,3RCH,3RBL,3RBF,9HRH,9HRI,1JS6,1JS3</td>\n",
       "      <td>vitamin b6</td>\n",
       "      <td>In this cohort of stage III colon cancer patie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Zanubrutinib</td>\n",
       "      <td>Tyrosine-protein kinase BTK</td>\n",
       "      <td>8GMB,4XI2,8FLG,6W07,6W06,6VXQ,6NZM,6HRT,6HRP,6...</td>\n",
       "      <td>zanubrutinib</td>\n",
       "      <td>The abstract lists zanubrutinib as one of seve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Drug                                          Receptor  \\\n",
       "0                Metformin                          Acetyl-CoA carboxylase 2   \n",
       "1                Cetuximab                  Epidermal growth factor receptor   \n",
       "2              Bevacizumab              Vascular endothelial growth factor A   \n",
       "3             Pioglitazone  Peroxisome proliferator-activated receptor gamma   \n",
       "4   Adenosine triphosphate                      Tyrosine-protein kinase ABL1   \n",
       "..                     ...                                               ...   \n",
       "63               Valproate                  Glycogen synthase kinase-3 alpha   \n",
       "64             Venlafaxine                             Serotonin transporter   \n",
       "65            Vildagliptin                            Dipeptidyl peptidase 4   \n",
       "66              Vitamin B6               Aromatic-L-amino-acid decarboxylase   \n",
       "67            Zanubrutinib                       Tyrosine-protein kinase BTK   \n",
       "\n",
       "                                               PDB_ID              report_key  \\\n",
       "0   3FF6,3TDC,2X24,3JRX,3JRW,2HJW,4HQ6,5KKN,3GLK,3...               metformin   \n",
       "1   7SZ7,7SZ5,7SYE,7SYD,7SZ1,7SZ0,8HGS,8HGP,8HGO,5...               cetuximab   \n",
       "2   3V2A,5T89,8UWZ,6T9D,7KF1,7KF0,7KEZ,5FV2,5FV1,3...             bevacizumab   \n",
       "3   3E00,3DZY,3DZU,7QB1,6L89,6K0T,6AD9,5HZC,5F9B,5...            pioglitazone   \n",
       "4   5MO4,1OPK,1OPL,2FO0,8SSN,4XEY,6XR7,6XR6,2E2B,4...  adenosine triphosphate   \n",
       "..                                                ...                     ...   \n",
       "63  7SXF,7SXG,8VMG,8VMF,8VME,4NM7,4NM5,5K5N,6TCU,3...               valproate   \n",
       "64  6VRL,6VRK,6VRH,7TXT,7LWD,5I6Z,6W2C,6W2B,6DZZ,6...             venlafaxine   \n",
       "65  2QTB,2QT9,2BGR,2JID,3F8S,2QJR,3W2T,3VJM,3VJL,3...            vildagliptin   \n",
       "66  9GNS,8ORA,8OR9,3RCH,3RBL,3RBF,9HRH,9HRI,1JS6,1JS3              vitamin b6   \n",
       "67  8GMB,4XI2,8FLG,6W07,6W06,6VXQ,6NZM,6HRT,6HRP,6...            zanubrutinib   \n",
       "\n",
       "                                               report  \n",
       "0   In this retrospective study of locally advance...  \n",
       "1   This case report links cetuximab (used as biol...  \n",
       "2   Bevacizumab was used in this metastatic CRC ca...  \n",
       "3   Pioglitazone, a PPARG (thiazolidinedione) agon...  \n",
       "4   The abstract notes tumor (including colorectal...  \n",
       "..                                                ...  \n",
       "63  The HDAC inhibitor sodium valproate reduced di...  \n",
       "64  The abstract reports that venlafaxine provides...  \n",
       "65  Vildagliptin, as a member of the DPPâ€‘4 inhibit...  \n",
       "66  In this cohort of stage III colon cancer patie...  \n",
       "67  The abstract lists zanubrutinib as one of seve...  \n",
       "\n",
       "[68 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "reports_dict = load_reports_dir(summaries_path)\n",
    "\n",
    "pat_drop_pmid = re.compile(r\"^PMID:.*$\", re.I | re.M)\n",
    "drug_data_clean[\"report\"] = drug_data_clean[\"report_key\"].map(reports_dict.get)\n",
    "\n",
    "\n",
    "drug_data_clean[\"report\"] = drug_data_clean[\"report\"].apply(\n",
    "    lambda txt: pat_drop_pmid.sub(\"\", txt).strip() if isinstance(txt, str) else txt\n",
    ")\n",
    "drug_data_clean = drug_data_clean.dropna(subset=[\"report\"]).reset_index(drop=True)\n",
    "print(drug_data_clean.shape)\n",
    "\n",
    "drug_data_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00aad130",
   "metadata": {},
   "source": [
    "### read train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "906e28e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 14\n"
     ]
    }
   ],
   "source": [
    "train_df_path = file_paths[\"train_val_paths\"][\"train_path\"]\n",
    "val_df_path = file_paths[\"train_val_paths\"][\"val_path\"]\n",
    "\n",
    "train_df = pd.read_csv(train_df_path)\n",
    "val_df = pd.read_csv(val_df_path)\n",
    "\n",
    "print(len(train_df), len(val_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607ae33e",
   "metadata": {},
   "source": [
    "## llm training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2db29f1-6150-4d34-84f3-91d347dbff26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0},          # <-- force all modules to GPU:0\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "print(\"Loaded on:\", next(model.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369dab85",
   "metadata": {},
   "source": [
    "### load model configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab3a2ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID   = PARAMS[\"model\"][\"id\"]\n",
    "MODEL_OUTPUT_DIR = PARAMS[\"model\"][\"output_dir\"] ## save model configs here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ee3e5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _dtype_from_str(s: str):\n",
    "    s = (s or \"\").lower()\n",
    "    if s in (\"bf16\", \"bfloat16\"):\n",
    "        return torch.bfloat16\n",
    "    if s in (\"fp16\", \"float16\", \"half\"):\n",
    "        return torch.float16\n",
    "    if s in (\"fp32\", \"float32\"):\n",
    "        return torch.float32\n",
    "    if s in (\"auto\", \"\"):\n",
    "        return None\n",
    "    raise ValueError(f\"Unknown dtype string: {s!r}\")\n",
    "\n",
    "\n",
    "def _normalize_max_memory(mm: Optional[Dict[Any, str]]) -> Optional[Dict[Any, str]]:\n",
    "    \"\"\"\n",
    "    - Convert string keys like \"0\" to int 0.\n",
    "    - Drop GPU entries if there is no CUDA device.\n",
    "    - Leave 'cpu' / 'mps' / 'disk' untouched.\n",
    "    \"\"\"\n",
    "    if not mm:\n",
    "        return None\n",
    "\n",
    "    has_cuda = torch.cuda.is_available() and torch.cuda.device_count() > 0\n",
    "    out: Dict[Any, str] = {}\n",
    "\n",
    "    for k, v in mm.items():\n",
    "        # \"0\" -> 0\n",
    "        if isinstance(k, str) and k.isdigit():\n",
    "            k_int = int(k)\n",
    "            if has_cuda and k_int < torch.cuda.device_count():\n",
    "                out[k_int] = v\n",
    "            # if no CUDA or out-of-range GPU id, just skip\n",
    "        else:\n",
    "            out[k] = v\n",
    "\n",
    "    return out or None\n",
    "    \n",
    "def configure_precision_flags(params: Dict[str, Any]) -> None:\n",
    "    has_cuda = torch.cuda.is_available()\n",
    "    supports_bf16 = has_cuda and torch.cuda.is_bf16_supported()\n",
    "\n",
    "    # Model / quantization dtype (for from_pretrained)\n",
    "    if supports_bf16:\n",
    "        params[\"model\"][\"dtype\"] = \"bf16\"\n",
    "        params[\"bnb\"][\"compute_dtype\"] = \"bf16\"\n",
    "    elif has_cuda:\n",
    "        params[\"model\"][\"dtype\"] = \"fp16\"\n",
    "        params[\"bnb\"][\"compute_dtype\"] = \"fp16\"\n",
    "    else:\n",
    "        # pure CPU: keep it fp32\n",
    "        params[\"model\"][\"dtype\"] = \"fp32\"\n",
    "        params[\"bnb\"][\"compute_dtype\"] = \"fp32\"\n",
    "\n",
    "    # TrainingArguments precision flags â€“ MUST be mutually exclusive\n",
    "    params[\"train\"][\"bf16\"] = bool(supports_bf16)\n",
    "    params[\"train\"][\"fp16\"] = bool(has_cuda and not supports_bf16)\n",
    "\n",
    "\n",
    "def load_dataframe(path: str):\n",
    "    df = pd.read_csv(path)\n",
    "    drop_cols = PARAMS[\"data\"][\"dropna_cols\"]\n",
    "    df = df.dropna(subset=drop_cols).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "class SafeDict(dict):\n",
    "    def __missing__(self, key):\n",
    "        return \"\"  # default blank for any missing placeholder\n",
    "\n",
    "def get_first_key(ex, keys):\n",
    "    \"\"\"Return first non-empty value among candidate keys from a row/example.\"\"\"\n",
    "    for k in keys:\n",
    "        if k in ex and ex[k] is not None and str(ex[k]).strip() != \"\":\n",
    "            return str(ex[k])\n",
    "    return \"\"\n",
    "\n",
    "def chat_format_map_fn(\n",
    "    ex: Dict[str, Any],\n",
    "    training_prompt: str,\n",
    "    system_prompt: str,\n",
    "    few_shots: List[Dict[str, str]],\n",
    "    tokenizer: AutoTokenizer,\n",
    ") -> Dict[str, str]:\n",
    "    vals = {\n",
    "        \"Drug\": ex.get(\"Drug\", \"\"),\n",
    "        \"Receptor\": ex.get(\"Receptor\", \"\"),\n",
    "        \"report\": ex.get(\"report\", \"\"),\n",
    "        \"PDB_ID\": ex.get(\"PDB_ID\", \"\"),\n",
    "    }\n",
    "    user_text = training_prompt.format_map(SafeDict(vals))\n",
    "\n",
    "    msgs: List[Dict[str, str]] = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "\n",
    "    for shot in few_shots or []:\n",
    "        if shot and \"user\" in shot and \"assistant\" in shot:\n",
    "            msgs.append({\"role\": \"user\", \"content\": shot[\"user\"]})\n",
    "            msgs.append({\"role\": \"assistant\", \"content\": shot[\"assistant\"]})\n",
    "\n",
    "    msgs.append({\"role\": \"user\", \"content\": user_text})\n",
    "\n",
    "    rendered = tokenizer.apply_chat_template(\n",
    "        msgs,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,  # produces assistant prefix; good for SFT\n",
    "    )\n",
    "    return {\"text\": rendered}\n",
    "\n",
    "\n",
    "\n",
    "def build_dataset(\n",
    "    csv_path: str,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    params: Dict[str, Any],\n",
    "    prompts: Dict[str, Any],\n",
    ") -> Dataset:\n",
    "    df = load_dataframe(csv_path)\n",
    "\n",
    "    training_prompt = prompts[\"prompts\"][\"train_user_template\"]\n",
    "    system_prompt = prompts[\"prompts\"][\"system\"]\n",
    "    use_chat_format = prompts.get(\"use_chat_format\", False)\n",
    "    few_shots = prompts.get(\"few_shots\", [])\n",
    "\n",
    "    cols = list(df.columns)\n",
    "    ds = Dataset.from_pandas(df, preserve_index=False)\n",
    "\n",
    "    if use_chat_format:\n",
    "        # HuggingFace Datasets map passes only the example dict, so we wrap\n",
    "        # our extra arguments using a closure-like helper via kwargs.\n",
    "        def _wrapped_chat_map_fn(ex: Dict[str, Any]) -> Dict[str, str]:\n",
    "            return chat_format_map_fn(\n",
    "                ex=ex,\n",
    "                training_prompt=training_prompt,\n",
    "                system_prompt=system_prompt,\n",
    "                few_shots=few_shots,\n",
    "                tokenizer=tokenizer,\n",
    "            )\n",
    "\n",
    "        ds = ds.map(_wrapped_chat_map_fn, remove_columns=cols)\n",
    "    else:\n",
    "        def _wrapped_plain_map_fn(ex: Dict[str, Any]) -> Dict[str, str]:\n",
    "            return plain_format_map_fn(\n",
    "                ex=ex,\n",
    "                training_prompt=training_prompt,\n",
    "            )\n",
    "\n",
    "        ds = ds.map(_wrapped_plain_map_fn, remove_columns=cols)\n",
    "\n",
    "    ds = ds.train_test_split(\n",
    "        test_size=params[\"data\"][\"test_size\"],\n",
    "        seed=params[\"data\"][\"seed\"],\n",
    "    )\n",
    "    return ds\n",
    "\n",
    "\n",
    "\n",
    "def get_model_and_tokenizer(params):\n",
    "    model_id = params[\"model\"][\"id\"]\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_id,\n",
    "        use_fast=True,\n",
    "        trust_remote_code=params[\"model\"].get(\"trust_remote_code\", True),\n",
    "    )\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # bitsandbytes / QLoRA config\n",
    "    bnb_cfg = None\n",
    "    if params[\"model\"].get(\"load_in_4bit\", False):\n",
    "        bnb_cfg = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=params[\"bnb\"][\"quant_type\"],\n",
    "            bnb_4bit_use_double_quant=params[\"bnb\"][\"double_quant\"],\n",
    "            bnb_4bit_compute_dtype=_dtype_from_str(params[\"bnb\"][\"compute_dtype\"]) or torch.float16,\n",
    "        )\n",
    "\n",
    "    # normalize max_memory from YAML\n",
    "    max_memory = _normalize_max_memory(params[\"model\"].get(\"max_memory\"))\n",
    "\n",
    "    # use dtype (new API) instead of torch_dtype\n",
    "    dtype = _dtype_from_str(params[\"model\"].get(\"dtype\"))\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=bnb_cfg,\n",
    "        device_map=\"auto\",\n",
    "        dtype=dtype,                           \n",
    "        max_memory=max_memory,\n",
    "        offload_folder=params[\"model\"][\"offload_folder\"],\n",
    "        low_cpu_mem_usage=True,\n",
    "        trust_remote_code=params[\"model\"][\"trust_remote_code\"],\n",
    "    )\n",
    "\n",
    "    model.config.use_cache = False\n",
    "    model.gradient_checkpointing_enable()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "\n",
    "def get_lora_wrapped(model, params: Dict[str, Any]):\n",
    "    lconf = params[\"lora\"]\n",
    "    lora_cfg = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        r=lconf[\"r\"],\n",
    "        lora_alpha=lconf[\"alpha\"],\n",
    "        lora_dropout=lconf[\"dropout\"],\n",
    "        target_modules=lconf[\"target_modules\"],\n",
    "        bias=lconf[\"bias\"],\n",
    "    )\n",
    "    model = get_peft_model(model, lora_cfg)\n",
    "    return model\n",
    "\n",
    "\n",
    "def main(\n",
    "    train_csv: str,\n",
    "    file_paths: Dict[str, Any],\n",
    "    PARAMS: Dict[str, Any],\n",
    "    prompts: Dict[str, Any],\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Run SFT training.\n",
    "\n",
    "    Args:\n",
    "        train_csv: Path to the training CSV.\n",
    "        file_paths: Config dict from P3-config.yaml (if you need more paths).\n",
    "        params: Training hyperparameters and model settings.\n",
    "        prompts: Prompt templates and chat-format toggles.\n",
    "    \"\"\"\n",
    "    model, tokenizer = get_model_and_tokenizer(PARAMS)\n",
    "    model = get_lora_wrapped(model, PARAMS)\n",
    "\n",
    "    ds = build_dataset(train_csv, tokenizer, PARAMS, prompts)\n",
    "\n",
    "\n",
    "    max_len = PARAMS[\"data\"][\"max_seq_length\"]\n",
    "\n",
    "    training_args = SFTConfig(\n",
    "        output_dir=PARAMS[\"model\"][\"output_dir\"],\n",
    "        num_train_epochs=PARAMS[\"train\"][\"epochs\"],\n",
    "        per_device_train_batch_size=PARAMS[\"train\"][\"per_device_train_batch_size\"],\n",
    "        per_device_eval_batch_size=PARAMS[\"train\"][\"per_device_eval_batch_size\"],\n",
    "        gradient_accumulation_steps=PARAMS[\"train\"][\"gradient_accumulation_steps\"],\n",
    "        learning_rate=float(PARAMS[\"train\"][\"learning_rate\"]),\n",
    "        lr_scheduler_type=PARAMS[\"train\"][\"scheduler\"],\n",
    "        warmup_ratio=float(PARAMS[\"train\"][\"warmup_ratio\"]),\n",
    "        weight_decay=float(PARAMS[\"train\"][\"weight_decay\"]),\n",
    "        logging_steps=PARAMS[\"train\"][\"logging_steps\"],\n",
    "        # eval_strategy=PARAMS[\"train\"][\"evaluation_strategy\"],   # <- NEW (replaces evaluation_strategy)\n",
    "        # eval_steps=PARAMS[\"train\"][\"eval_steps\"],\n",
    "        save_steps=PARAMS[\"train\"][\"save_steps\"],\n",
    "        save_total_limit=PARAMS[\"train\"][\"save_total_limit\"],\n",
    "        bf16=PARAMS[\"train\"][\"bf16\"],\n",
    "        fp16=PARAMS[\"train\"][\"fp16\"],\n",
    "        gradient_checkpointing=PARAMS[\"train\"][\"gradient_checkpointing\"],\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": False}, # <- NEW to silence Torch 2.5 warning\n",
    "        packing=PARAMS[\"data\"][\"packing\"],\n",
    "        optim=PARAMS[\"train\"][\"optim\"],\n",
    "        max_grad_norm=PARAMS[\"train\"][\"max_grad_norm\"],\n",
    "        seed=PARAMS[\"train\"][\"seed\"],\n",
    "        report_to=PARAMS[\"train\"][\"report_to\"],\n",
    "        load_best_model_at_end=PARAMS[\"train\"][\"load_best_model\"],\n",
    "    \n",
    "        # Move these here (donâ€™t pass them to SFTTrainer):\n",
    "        # max_seq_length=PARAMS[\"data\"][\"max_seq_length\"],\n",
    "        dataset_text_field=\"text\",\n",
    "    )\n",
    "\n",
    "        \n",
    "    # early_stopping = EarlyStoppingCallback(\n",
    "    #     early_stopping_patience=1,      # VERY small because only 3 epochs\n",
    "    #     early_stopping_threshold=0.0\n",
    "    # )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,\n",
    "        train_dataset=ds[\"train\"],\n",
    "        args=training_args,         # <- contains max_seq_length + dataset_text_field\n",
    "        # callbacks=[early_stopping],\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model()\n",
    "    tokenizer.save_pretrained(PARAMS[\"model\"][\"output_dir\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d75ab7af-6d10-4dfc-bddd-bc4d028a450f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Default to train_path from config, but you can pass any CSV you want\u001b[39;00m\n\u001b[1;32m     11\u001b[0m train_csv_path \u001b[38;5;241m=\u001b[39m file_paths[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_val_paths\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 13\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_csv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_csv_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_paths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mPARAMS\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPARAMS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 228\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(train_csv, file_paths, PARAMS, prompts)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmain\u001b[39m(\n\u001b[1;32m    214\u001b[0m     train_csv: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    215\u001b[0m     file_paths: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m    216\u001b[0m     PARAMS: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m    217\u001b[0m     prompts: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m    218\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03m    Run SFT training.\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;124;03m        prompts: Prompt templates and chat-format toggles.\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 228\u001b[0m     model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mget_model_and_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPARAMS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m     model \u001b[38;5;241m=\u001b[39m get_lora_wrapped(model, PARAMS)\n\u001b[1;32m    231\u001b[0m     ds \u001b[38;5;241m=\u001b[39m build_dataset(train_csv, tokenizer, PARAMS, prompts)\n",
      "Cell \u001b[0;32mIn[24], line 181\u001b[0m, in \u001b[0;36mget_model_and_tokenizer\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m    178\u001b[0m max_memory \u001b[38;5;241m=\u001b[39m _normalize_max_memory(params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_memory\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    179\u001b[0m torch_dtype \u001b[38;5;241m=\u001b[39m _dtype_from_str(params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m--> 181\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# ðŸ”‘ still required\u001b[39;49;00m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moffload_folder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrust_remote_code\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m# CRITICAL for training\u001b[39;00m\n\u001b[1;32m    193\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/fyp_env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:373\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    372\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[0;32m--> 373\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    379\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/fyp_env/lib/python3.10/site-packages/transformers/modeling_utils.py:248\u001b[0m, in \u001b[0;36mrestore_default_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    246\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    250\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m~/Documents/fyp_env/lib/python3.10/site-packages/transformers/modeling_utils.py:3998\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3996\u001b[0m \u001b[38;5;66;03m# Prepare the full device map\u001b[39;00m\n\u001b[1;32m   3997\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3998\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m \u001b[43m_get_device_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4000\u001b[0m \u001b[38;5;66;03m# restore default dtype\u001b[39;00m\n\u001b[1;32m   4001\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/fyp_env/lib/python3.10/site-packages/transformers/integrations/accelerate.py:428\u001b[0m, in \u001b[0;36m_get_device_map\u001b[0;34m(model, device_map, max_memory, hf_quantizer)\u001b[0m\n\u001b[1;32m    420\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m infer_auto_device_map(\n\u001b[1;32m    421\u001b[0m         model,\n\u001b[1;32m    422\u001b[0m         max_memory\u001b[38;5;241m=\u001b[39minferred_max_memory,\n\u001b[1;32m    423\u001b[0m         no_split_module_classes\u001b[38;5;241m=\u001b[39mno_split_modules,\n\u001b[1;32m    424\u001b[0m         hf_quantizer\u001b[38;5;241m=\u001b[39mhf_quantizer,\n\u001b[1;32m    425\u001b[0m     )\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 428\u001b[0m         \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m device_map\n",
      "File \u001b[0;32m~/Documents/fyp_env/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:81\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(device_map\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m values \u001b[38;5;241m!=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m} \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m values \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m values):\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`from_pretrained`. Check \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor more details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load configs from YAML\n",
    "    file_paths = load_config(yaml_path=\"P3-config.yaml\")\n",
    "    PARAMS = load_config(yaml_path=\"training_params.yaml\")\n",
    "    prompts = load_config(yaml_path=\"prompts.yaml\")\n",
    "\n",
    "    configure_precision_flags(PARAMS)\n",
    "\n",
    "\n",
    "    # Default to train_path from config, but you can pass any CSV you want\n",
    "    train_csv_path = file_paths[\"train_val_paths\"][\"train_path\"]\n",
    "\n",
    "    main(\n",
    "        train_csv=train_csv_path,\n",
    "        file_paths=file_paths,\n",
    "        PARAMS=PARAMS,\n",
    "        prompts=prompts,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0799a8a-fd2e-40d2-849f-4a3cff78e049",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fypkernel2",
   "language": "python",
   "name": "fyp_kernel2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
