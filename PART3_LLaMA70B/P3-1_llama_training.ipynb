{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf20fdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Standard Library\n",
    "# ================================\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "# ================================\n",
    "# Third-Party Libraries\n",
    "# ================================\n",
    "import bitsandbytes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import yaml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "# HuggingFace Transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback,\n",
    "    GenerationConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    __version__ as HF_VER,\n",
    ")\n",
    "\n",
    "# PEFT (LoRA / QLoRA)\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    TaskType,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "\n",
    "# TRL (Supervised Fine-Tuning Trainer)\n",
    "from trl import SFTConfig, SFTTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ec2cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(yaml_path):\n",
    "    with open(yaml_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "    \n",
    "file_paths = load_config(yaml_path=\"P3-config.yaml\")\n",
    "PARAMS = load_config(yaml_path=\"training_params.yaml\")\n",
    "prompts = load_config(yaml_path=\"prompts.yaml\")\n",
    "\n",
    "dtype = \"bf16\" if torch.cuda.is_bf16_supported() else \"fp16\"\n",
    "PARAMS[\"model\"][\"dtype\"] = dtype\n",
    "PARAMS[\"bnb\"][\"compute_dtype\"] = dtype\n",
    "PARAMS[\"train\"][\"bf16\"] = torch.cuda.is_bf16_supported()\n",
    "PARAMS[\"train\"][\"fp16\"] = not torch.cuda.is_bf16_supported()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1931fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "drug_data_path = file_paths[\"input_file_paths\"][\"drug_data_path\"]\n",
    "\n",
    "drug_data_df = pd.read_csv(drug_data_path)\n",
    "drug_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb694ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "summaries_path = file_paths[\"input_file_paths\"][\"summaries_path\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bfab04",
   "metadata": {},
   "source": [
    "## Data cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9c563f",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_data_clean = drug_data_df.copy()\n",
    "drug_data_clean.columns\n",
    "\n",
    "drug_data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e1f672",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _norm(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", str(s).strip().upper())\n",
    "\n",
    "def _safe(s: Optional[str]) -> str:\n",
    "    return \"\" if s is None else str(s).strip()\n",
    "\n",
    "def _read_text_file(path: str) -> Optional[str]:\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            txt = f.read().strip()\n",
    "            return txt if txt else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _canon(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Lowercase; turn any run of non-alphanumerics (incl. underscores, hyphens, commas)\n",
    "    into a single space; then collapse spaces.\n",
    "    \"\"\"\n",
    "    s = str(s).lower().strip()\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \" \", s)   # underscores, hyphens, slashes -> space\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "\n",
    "def load_reports_dir(reports_dir: str) -> Dict[str, str]:\n",
    "    pat_file = re.compile(r\"^(.+?)_summary\\.txt$\", re.I)\n",
    "    pat_rm   = re.compile(r\"PMID:\\s*\\d+\\s*(?:\\n\\s*)?(?:no relevant information found\\.?)\",\n",
    "                          re.I)\n",
    "\n",
    "    out = {}\n",
    "\n",
    "    for p in glob.glob(os.path.join(reports_dir, \"*_summary.txt\")):\n",
    "        fn = os.path.basename(p)\n",
    "        m = pat_file.match(fn)\n",
    "        if not m:\n",
    "            continue\n",
    "\n",
    "        key = _canon(m.group(1))\n",
    "\n",
    "\n",
    "        txt = _read_text_file(p)\n",
    "        if not txt:\n",
    "            continue\n",
    "\n",
    "        # remove junk pmid blocks\n",
    "        txt = pat_rm.sub(\"\", txt)\n",
    "\n",
    "        # collapse lines\n",
    "        txt = re.sub(r\"\\n{3,}\", \"\\n\\n\", txt).strip()\n",
    "\n",
    "        # *** reject empty after cleaning ***\n",
    "        if not txt:\n",
    "            # optional debug: print(f\"SKIP blank: {fn}\")\n",
    "            continue\n",
    "\n",
    "        out[key] = txt\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b02f4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_dict = load_reports_dir(summaries_path)\n",
    "\n",
    "for i, (drug, text) in enumerate(reports_dict.items()):\n",
    "    print(f\"{i:02d} | {drug}\")\n",
    "    print(text[:200], \"...\\n\")      # first 200 chars preview\n",
    "\n",
    "    if i> 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59afbaa6",
   "metadata": {},
   "source": [
    "## merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c7dd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## map drug report name to actual drug name from dataset\n",
    "drug_data_clean[\"report_key\"] = drug_data_clean[\"Drug\"].map(_canon)\n",
    "drug_data_clean[\"report\"] = drug_data_clean[\"report_key\"].map(lambda k: reports_dict.get(k))\n",
    "print(\"---- DRUG → report_key → exists ----\")\n",
    "for d, k in drug_data_clean[[\"Drug\",\"report_key\"]].head(10).values:\n",
    "    print(f\"{d:<35} → {k:<35} → {k in reports_dict}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178512f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reports_dict = load_reports_dir(summaries_path)\n",
    "\n",
    "pat_drop_pmid = re.compile(r\"^PMID:.*$\", re.I | re.M)\n",
    "drug_data_clean[\"report\"] = drug_data_clean[\"report_key\"].map(reports_dict.get)\n",
    "\n",
    "\n",
    "drug_data_clean[\"report\"] = drug_data_clean[\"report\"].apply(\n",
    "    lambda txt: pat_drop_pmid.sub(\"\", txt).strip() if isinstance(txt, str) else txt\n",
    ")\n",
    "drug_data_clean = drug_data_clean.dropna(subset=[\"report\"]).reset_index(drop=True)\n",
    "print(drug_data_clean.shape)\n",
    "\n",
    "drug_data_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00aad130",
   "metadata": {},
   "source": [
    "### read train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906e28e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_path = file_paths[\"train_val_paths\"][\"train_path\"]\n",
    "val_df_path = file_paths[\"train_val_paths\"][\"val_path\"]\n",
    "\n",
    "train_df = pd.read_csv(train_df_path)\n",
    "val_df = pd.read_csv(val_df_path)\n",
    "\n",
    "print(len(train_df), len(val_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607ae33e",
   "metadata": {},
   "source": [
    "## llm training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369dab85",
   "metadata": {},
   "source": [
    "### load model configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3a2ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID   = PARAMS[\"model\"][\"id\"]\n",
    "MODEL_OUTPUT_DIR = PARAMS[\"model\"][\"output_dir\"] ## save model configs here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee3e5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _dtype_from_str(s: str):\n",
    "    s = (s or \"\").lower()\n",
    "    if s in (\"bf16\", \"bfloat16\"):\n",
    "        return torch.bfloat16\n",
    "    if s in (\"fp16\", \"float16\", \"half\"):\n",
    "        return torch.float16\n",
    "    if s in (\"fp32\", \"float32\"):\n",
    "        return torch.float32\n",
    "    if s in (\"auto\", \"\"):\n",
    "        return None\n",
    "    raise ValueError(f\"Unknown dtype string: {s!r}\")\n",
    "\n",
    "\n",
    "def _normalize_max_memory(mm: Optional[Dict[Any, str]]) -> Optional[Dict[Any, str]]:\n",
    "    \"\"\"\n",
    "    - Convert string keys like \"0\" to int 0.\n",
    "    - Drop GPU entries if there is no CUDA device.\n",
    "    - Leave 'cpu' / 'mps' / 'disk' untouched.\n",
    "    \"\"\"\n",
    "    if not mm:\n",
    "        return None\n",
    "\n",
    "    has_cuda = torch.cuda.is_available() and torch.cuda.device_count() > 0\n",
    "    out: Dict[Any, str] = {}\n",
    "\n",
    "    for k, v in mm.items():\n",
    "        # \"0\" -> 0\n",
    "        if isinstance(k, str) and k.isdigit():\n",
    "            k_int = int(k)\n",
    "            if has_cuda and k_int < torch.cuda.device_count():\n",
    "                out[k_int] = v\n",
    "            # if no CUDA or out-of-range GPU id, just skip\n",
    "        else:\n",
    "            out[k] = v\n",
    "\n",
    "    return out or None\n",
    "    \n",
    "def configure_precision_flags(params: Dict[str, Any]) -> None:\n",
    "    has_cuda = torch.cuda.is_available()\n",
    "    supports_bf16 = has_cuda and torch.cuda.is_bf16_supported()\n",
    "\n",
    "    # Model / quantization dtype (for from_pretrained)\n",
    "    if supports_bf16:\n",
    "        params[\"model\"][\"dtype\"] = \"bf16\"\n",
    "        params[\"bnb\"][\"compute_dtype\"] = \"bf16\"\n",
    "    elif has_cuda:\n",
    "        params[\"model\"][\"dtype\"] = \"fp16\"\n",
    "        params[\"bnb\"][\"compute_dtype\"] = \"fp16\"\n",
    "    else:\n",
    "        # pure CPU: keep it fp32\n",
    "        params[\"model\"][\"dtype\"] = \"fp32\"\n",
    "        params[\"bnb\"][\"compute_dtype\"] = \"fp32\"\n",
    "\n",
    "    # TrainingArguments precision flags – MUST be mutually exclusive\n",
    "    params[\"train\"][\"bf16\"] = bool(supports_bf16)\n",
    "    params[\"train\"][\"fp16\"] = bool(has_cuda and not supports_bf16)\n",
    "\n",
    "\n",
    "def load_dataframe(path: str):\n",
    "    df = pd.read_csv(path)\n",
    "    drop_cols = PARAMS[\"data\"][\"dropna_cols\"]\n",
    "    df = df.dropna(subset=drop_cols).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "class SafeDict(dict):\n",
    "    def __missing__(self, key):\n",
    "        return \"\"  # default blank for any missing placeholder\n",
    "\n",
    "def get_first_key(ex, keys):\n",
    "    \"\"\"Return first non-empty value among candidate keys from a row/example.\"\"\"\n",
    "    for k in keys:\n",
    "        if k in ex and ex[k] is not None and str(ex[k]).strip() != \"\":\n",
    "            return str(ex[k])\n",
    "    return \"\"\n",
    "\n",
    "def chat_format_map_fn(\n",
    "    ex: Dict[str, Any],\n",
    "    training_prompt: str,\n",
    "    system_prompt: str,\n",
    "    few_shots: List[Dict[str, str]],\n",
    "    tokenizer: AutoTokenizer,\n",
    ") -> Dict[str, str]:\n",
    "    vals = {\n",
    "        \"Drug\": ex.get(\"Drug\", \"\"),\n",
    "        \"Receptor\": ex.get(\"Receptor\", \"\"),\n",
    "        \"report\": ex.get(\"report\", \"\"),\n",
    "        \"PDB_ID\": ex.get(\"PDB_ID\", \"\"),\n",
    "    }\n",
    "    user_text = training_prompt.format_map(SafeDict(vals))\n",
    "\n",
    "    msgs: List[Dict[str, str]] = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "\n",
    "    for shot in few_shots or []:\n",
    "        if shot and \"user\" in shot and \"assistant\" in shot:\n",
    "            msgs.append({\"role\": \"user\", \"content\": shot[\"user\"]})\n",
    "            msgs.append({\"role\": \"assistant\", \"content\": shot[\"assistant\"]})\n",
    "\n",
    "    msgs.append({\"role\": \"user\", \"content\": user_text})\n",
    "\n",
    "    rendered = tokenizer.apply_chat_template(\n",
    "        msgs,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,  # produces assistant prefix; good for SFT\n",
    "    )\n",
    "    return {\"text\": rendered}\n",
    "\n",
    "\n",
    "\n",
    "def build_dataset(\n",
    "    csv_path: str,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    params: Dict[str, Any],\n",
    "    prompts: Dict[str, Any],\n",
    ") -> Dataset:\n",
    "    df = load_dataframe(csv_path)\n",
    "\n",
    "    training_prompt = prompts[\"prompts\"][\"train_user_template\"]\n",
    "    system_prompt = prompts[\"prompts\"][\"system\"]\n",
    "    use_chat_format = prompts.get(\"use_chat_format\", False)\n",
    "    few_shots = prompts.get(\"few_shots\", [])\n",
    "\n",
    "    cols = list(df.columns)\n",
    "    ds = Dataset.from_pandas(df, preserve_index=False)\n",
    "\n",
    "    if use_chat_format:\n",
    "        # HuggingFace Datasets map passes only the example dict, so we wrap\n",
    "        # our extra arguments using a closure-like helper via kwargs.\n",
    "        def _wrapped_chat_map_fn(ex: Dict[str, Any]) -> Dict[str, str]:\n",
    "            return chat_format_map_fn(\n",
    "                ex=ex,\n",
    "                training_prompt=training_prompt,\n",
    "                system_prompt=system_prompt,\n",
    "                few_shots=few_shots,\n",
    "                tokenizer=tokenizer,\n",
    "            )\n",
    "\n",
    "        ds = ds.map(_wrapped_chat_map_fn, remove_columns=cols)\n",
    "    else:\n",
    "        def _wrapped_plain_map_fn(ex: Dict[str, Any]) -> Dict[str, str]:\n",
    "            return plain_format_map_fn(\n",
    "                ex=ex,\n",
    "                training_prompt=training_prompt,\n",
    "            )\n",
    "\n",
    "        ds = ds.map(_wrapped_plain_map_fn, remove_columns=cols)\n",
    "\n",
    "    ds = ds.train_test_split(\n",
    "        test_size=params[\"data\"][\"test_size\"],\n",
    "        seed=params[\"data\"][\"seed\"],\n",
    "    )\n",
    "    return ds\n",
    "\n",
    "\n",
    "\n",
    "def get_model_and_tokenizer(params):\n",
    "    model_id = params[\"model\"][\"id\"]\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_id,\n",
    "        use_fast=True,\n",
    "        trust_remote_code=params[\"model\"].get(\"trust_remote_code\", True),\n",
    "    )\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # bitsandbytes / QLoRA config\n",
    "    bnb_cfg = None\n",
    "    if params[\"model\"].get(\"load_in_4bit\", False):\n",
    "        bnb_cfg = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=params[\"bnb\"][\"quant_type\"],\n",
    "            bnb_4bit_use_double_quant=params[\"bnb\"][\"double_quant\"],\n",
    "            bnb_4bit_compute_dtype=_dtype_from_str(params[\"bnb\"][\"compute_dtype\"]) or torch.float16,\n",
    "        )\n",
    "\n",
    "    # normalize max_memory from YAML\n",
    "    max_memory = _normalize_max_memory(params[\"model\"].get(\"max_memory\"))\n",
    "\n",
    "    # use dtype (new API) instead of torch_dtype\n",
    "    dtype = _dtype_from_str(params[\"model\"].get(\"dtype\"))\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=bnb_cfg,\n",
    "        device_map=\"auto\",\n",
    "        dtype=dtype,                           \n",
    "        max_memory=max_memory,\n",
    "        offload_folder=params[\"model\"][\"offload_folder\"],\n",
    "        low_cpu_mem_usage=True,\n",
    "        trust_remote_code=params[\"model\"][\"trust_remote_code\"],\n",
    "    )\n",
    "\n",
    "    model.config.use_cache = False\n",
    "    model.gradient_checkpointing_enable()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def get_lora_wrapped(model, params: Dict[str, Any]):\n",
    "    lconf = params[\"lora\"]\n",
    "    lora_cfg = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        r=lconf[\"r\"],\n",
    "        lora_alpha=lconf[\"alpha\"],\n",
    "        lora_dropout=lconf[\"dropout\"],\n",
    "        target_modules=lconf[\"target_modules\"],\n",
    "        bias=lconf[\"bias\"],\n",
    "    )\n",
    "    model = get_peft_model(model, lora_cfg)\n",
    "    return model\n",
    "\n",
    "\n",
    "def main(\n",
    "    train_csv: str,\n",
    "    file_paths: Dict[str, Any],\n",
    "    PARAMS: Dict[str, Any],\n",
    "    prompts: Dict[str, Any],\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Run SFT training.\n",
    "\n",
    "    Args:\n",
    "        train_csv: Path to the training CSV.\n",
    "        file_paths: Config dict from P3-config.yaml (if you need more paths).\n",
    "        params: Training hyperparameters and model settings.\n",
    "        prompts: Prompt templates and chat-format toggles.\n",
    "    \"\"\"\n",
    "    model, tokenizer = get_model_and_tokenizer(PARAMS)\n",
    "    model = get_lora_wrapped(model, PARAMS)\n",
    "\n",
    "    ds = build_dataset(train_csv, tokenizer, PARAMS, prompts)\n",
    "\n",
    "\n",
    "    max_len = PARAMS[\"data\"][\"max_seq_length\"]\n",
    "\n",
    "    training_args = SFTConfig(\n",
    "        output_dir=PARAMS[\"model\"][\"output_dir\"],\n",
    "        num_train_epochs=PARAMS[\"train\"][\"epochs\"],\n",
    "        per_device_train_batch_size=PARAMS[\"train\"][\"per_device_train_batch_size\"],\n",
    "        per_device_eval_batch_size=PARAMS[\"train\"][\"per_device_eval_batch_size\"],\n",
    "        gradient_accumulation_steps=PARAMS[\"train\"][\"gradient_accumulation_steps\"],\n",
    "        learning_rate=float(PARAMS[\"train\"][\"learning_rate\"]),\n",
    "        lr_scheduler_type=PARAMS[\"train\"][\"scheduler\"],\n",
    "        warmup_ratio=float(PARAMS[\"train\"][\"warmup_ratio\"]),\n",
    "        weight_decay=float(PARAMS[\"train\"][\"weight_decay\"]),\n",
    "        logging_steps=PARAMS[\"train\"][\"logging_steps\"],\n",
    "        # eval_strategy=PARAMS[\"train\"][\"evaluation_strategy\"],   # <- NEW (replaces evaluation_strategy)\n",
    "        # eval_steps=PARAMS[\"train\"][\"eval_steps\"],\n",
    "        save_steps=PARAMS[\"train\"][\"save_steps\"],\n",
    "        save_total_limit=PARAMS[\"train\"][\"save_total_limit\"],\n",
    "        bf16=PARAMS[\"train\"][\"bf16\"],\n",
    "        fp16=PARAMS[\"train\"][\"fp16\"],\n",
    "        gradient_checkpointing=PARAMS[\"train\"][\"gradient_checkpointing\"],\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": False}, # <- NEW to silence Torch 2.5 warning\n",
    "        packing=PARAMS[\"data\"][\"packing\"],\n",
    "        optim=PARAMS[\"train\"][\"optim\"],\n",
    "        max_grad_norm=PARAMS[\"train\"][\"max_grad_norm\"],\n",
    "        seed=PARAMS[\"train\"][\"seed\"],\n",
    "        report_to=PARAMS[\"train\"][\"report_to\"],\n",
    "        load_best_model_at_end=PARAMS[\"train\"][\"load_best_model\"],\n",
    "    \n",
    "        # Move these here (don’t pass them to SFTTrainer):\n",
    "        # max_seq_length=PARAMS[\"data\"][\"max_seq_length\"],\n",
    "        dataset_text_field=\"text\",\n",
    "    )\n",
    "\n",
    "        \n",
    "    # early_stopping = EarlyStoppingCallback(\n",
    "    #     early_stopping_patience=1,      # VERY small because only 3 epochs\n",
    "    #     early_stopping_threshold=0.0\n",
    "    # )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,\n",
    "        train_dataset=ds[\"train\"],\n",
    "        args=training_args,         # <- contains max_seq_length + dataset_text_field\n",
    "        # callbacks=[early_stopping],\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model()\n",
    "    tokenizer.save_pretrained(PARAMS[\"model\"][\"output_dir\"])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
