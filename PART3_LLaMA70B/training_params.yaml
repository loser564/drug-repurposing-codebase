model:
  # NOTE: requires HF access approval for meta-llama models.
  # Swap to "meta-llama/Llama-3.3-70B" if you want the base (non-instruct) variant.
  id: "meta-llama/Llama-3.3-70B-Instruct"
  output_dir: "llama-70b-qlora-finetune"
  load_in_4bit: true
  dtype: auto

  # 36GB VRAM target + CPU offload headroom
  max_memory:
    "0": "36GiB"
    "cpu": "96GiB"
  offload_folder: "offload_cache"
  trust_remote_code: true

bnb:
  # Standard QLoRA recipe
  quant_type: "nf4"          # nf4 | fp4
  double_quant: true
  compute_dtype: auto

data:
  dropna_cols:
    - "Drug"
    - "Receptor"
    - "report"
  test_size: 0.2
  seed: 42

  # 70B on 36GB: start conservative. If stable, try 1536 or 2048.
  max_seq_length: 1024
  packing: false

lora:
  # Keep LoRA small for 70B to stay within VRAM (you can increase later)
  r: 8
  alpha: 16
  dropout: 0.05
  bias: "none"

  # Llama-family linear modules (works for Llama 2/3/3.1/3.3)
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"

train:
  epochs: 3
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1

  # Lower than your 7B config to reduce activation pressure on 70B
  gradient_accumulation_steps: 8

  # Often safer for large QLoRA runs than 2e-4 (start here; tune later)
  learning_rate: 1e-4
  scheduler: "constant"
  warmup_ratio: 0.03
  weight_decay: 0.001
  optim: "paged_adamw_32bit"

  evaluation_strategy: "no"
  eval_steps: 200
  save_steps: 50
  save_total_limit: 2
  logging_steps: 25

  bf16: "auto"
  fp16: "auto"
  gradient_checkpointing: true
  max_grad_norm: 1.0
  seed: 42
  report_to: "none"
  load_best_model: false

gen:
  temperature: 0.2
  top_p: 0.9
  top_k: 50
  max_new_tokens: 1024
  do_sample: true
