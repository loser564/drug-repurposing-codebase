{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042b2f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Standard Library\n",
    "# ================================\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "# ================================\n",
    "# Third-Party Libraries\n",
    "# ================================\n",
    "import bitsandbytes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import yaml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "# HuggingFace Transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback,\n",
    "    GenerationConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    __version__ as HF_VER,\n",
    ")\n",
    "\n",
    "# PEFT (LoRA / QLoRA)\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    TaskType,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "\n",
    "# TRL (Supervised Fine-Tuning Trainer)\n",
    "from trl import SFTConfig, SFTTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e73b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(yaml_path):\n",
    "    with open(yaml_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "    \n",
    "file_paths = load_config(yaml_path=\"P3-config.yaml\")\n",
    "PARAMS = load_config(yaml_path=\"validation_params.yaml\")\n",
    "prompts = load_config(yaml_path=\"prompts.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5ea3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_path = file_paths['test_paths']['test_path']\n",
    "test_df = pd.read_csv(test_df_path)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064131d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _dtype_from_str(s: str):\n",
    "    s = (s or \"\").lower()\n",
    "    if s in (\"bf16\", \"bfloat16\"):\n",
    "        return torch.bfloat16\n",
    "    if s in (\"fp16\", \"float16\", \"half\"):\n",
    "        return torch.float16\n",
    "    if s in (\"fp32\", \"float32\"):\n",
    "        return torch.float32\n",
    "    return torch.float16\n",
    "\n",
    "\n",
    "def make_bnb_config(infer_cfg: Dict[str, Any]) -> BitsAndBytesConfig:\n",
    "    \"\"\"\n",
    "    Build BitsAndBytesConfig from validation_params.yaml's `quantization` block.\n",
    "    \"\"\"\n",
    "    q = infer_cfg[\"quantization\"]\n",
    "    compute_dtype = q.get(\"compute_dtype\", \"auto\")\n",
    "\n",
    "    # allow \"auto\" in YAML, resolve here\n",
    "    if compute_dtype == \"auto\":\n",
    "        compute_dtype = \"bf16\" if torch.cuda.is_bf16_supported() else \"fp16\"\n",
    "\n",
    "    return BitsAndBytesConfig(\n",
    "        load_in_4bit=q[\"load_in_4bit\"],\n",
    "        bnb_4bit_quant_type=q[\"quant_type\"],\n",
    "        bnb_4bit_use_double_quant=q[\"double_quant\"],\n",
    "        bnb_4bit_compute_dtype=_dtype_from_str(compute_dtype),\n",
    "    )\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Model loading\n",
    "# -------------------------------\n",
    "\n",
    "def load_model_and_tokenizer_for_inference(\n",
    "    infer_cfg: Dict[str, Any]\n",
    "):\n",
    "    \"\"\"\n",
    "    Load base model + LoRA adapter for inference.\n",
    "\n",
    "    Uses:\n",
    "      - 4-bit quantization (preferred) OR\n",
    "      - merged 16-bit model if `merge_16bit` is True.\n",
    "    \"\"\"\n",
    "    base_id = infer_cfg[\"base_model_id\"]\n",
    "    adapter_dir = infer_cfg[\"adapter_dir\"]\n",
    "    device_map = infer_cfg[\"batch\"][\"device_map\"]\n",
    "    merge_16bit = infer_cfg[\"merge_16bit\"]\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        base_id,\n",
    "        use_fast=True,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    if merge_16bit:\n",
    "        # 16-bit merged model\n",
    "        dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_id,\n",
    "            device_map=device_map,\n",
    "            torch_dtype=dtype,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        model = PeftModel.from_pretrained(model, adapter_dir)\n",
    "        model = model.merge_and_unload()\n",
    "    else:\n",
    "        # 4-bit quantized path\n",
    "        bnb_cfg = make_bnb_config(infer_cfg)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_id,\n",
    "            quantization_config=bnb_cfg,\n",
    "            device_map=device_map,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        model = PeftModel.from_pretrained(model, adapter_dir)\n",
    "\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Prompt composition\n",
    "# -------------------------------\n",
    "def render_user_template(user_template: str, drug: str, receptor: str, pdb_ids: str) -> str:\n",
    "    \"\"\"\n",
    "    Safely substitute only {Drug}, {Receptor}, and {PDB_ID} without\n",
    "    treating other braces in the template (e.g. JSON) as format fields.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        user_template\n",
    "        .replace(\"{Drug}\", drug or \"\")\n",
    "        .replace(\"{Receptor}\", receptor or \"\")\n",
    "        .replace(\"{PDB_ID}\", pdb_ids or \"\")\n",
    "    )\n",
    "    \n",
    "def select_user_template(p_cfg: Dict[str, Any], test: bool, tier: str) -> str:\n",
    "    \"\"\"\n",
    "    tier: one of {\"basic\", \"oncologist\", \"reviewer\"} for test.\n",
    "    For validation we ignore tier and use val_user_template.\n",
    "    \"\"\"\n",
    "    if not test:\n",
    "        return p_cfg[\"val_user_template\"]\n",
    "\n",
    "    # test mode\n",
    "    if tier == \"basic\":\n",
    "        return p_cfg[\"test_user_template\"]\n",
    "    if tier == \"oncologist\":\n",
    "        return p_cfg[\"test_user_template_oncologist\"]\n",
    "    if tier == \"reviewer\":\n",
    "        return p_cfg[\"test_user_template_reviewer\"]\n",
    "\n",
    "    raise ValueError(f\"Unknown prompt tier: {tier}\")\n",
    "\n",
    "\n",
    "def compose_chat_prompt(\n",
    "    drug: str,\n",
    "    receptor: str,\n",
    "    pdb_ids: str,\n",
    "    system_prompt: str,\n",
    "    user_template: str,\n",
    "    tokenizer,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Construct a chat-style prompt (same style as training),\n",
    "    folding the system prompt into the first user turn.\n",
    "    \"\"\"\n",
    "    user_text = render_user_template(\n",
    "        user_template=user_template,\n",
    "        drug=drug,\n",
    "        receptor=receptor,\n",
    "        pdb_ids=pdb_ids,\n",
    "    )\n",
    "\n",
    "\n",
    "    first_user = f\"<<SYS>>{system_prompt}<</SYS>>\\n\\n{user_text}\"\n",
    "    msgs = [{\"role\": \"user\", \"content\": first_user}]\n",
    "\n",
    "    rendered = tokenizer.apply_chat_template(\n",
    "        msgs,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    return rendered\n",
    "\n",
    "\n",
    "def compose_plain_prompt(\n",
    "    drug: str,\n",
    "    receptor: str,\n",
    "    pdb_ids: str,\n",
    "    user_template: str,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Construct a plain text prompt (no chat template).\n",
    "    \"\"\"\n",
    "    return user_template.format(\n",
    "        Drug=drug or \"\",\n",
    "        Receptor=receptor or \"\",\n",
    "        PDB_ID=pdb_ids or \"\",\n",
    "    )\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Row-level generation\n",
    "# -------------------------------\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_for_row(\n",
    "    row: Dict[str, Any],\n",
    "    model,\n",
    "    tokenizer,\n",
    "    infer_cfg: Dict[str, Any],\n",
    "    prompts_cfg: Dict[str, Any],\n",
    "    test: bool,\n",
    "    tier: str\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate a report for a single row.\n",
    "\n",
    "    `prompts_cfg` is expected to be the loaded prompts.yaml dict.\n",
    "    We select either the validation or test user template based on `test`.\n",
    "    \"\"\"\n",
    "    col_cfg = infer_cfg[\"columns\"]\n",
    "    col_d = col_cfg[\"drug\"]\n",
    "    col_r = col_cfg[\"receptor\"]\n",
    "    col_p = col_cfg[\"pdb\"]\n",
    "\n",
    "    drug = str(row.get(col_d, \"\") or \"\")\n",
    "    receptor = str(row.get(col_r, \"\") or \"\")\n",
    "\n",
    "    if col_p in row and pd.notna(row[col_p]):\n",
    "        pdb_raw = str(row[col_p])\n",
    "    else:\n",
    "        pdb_raw = \"\"\n",
    "\n",
    "    pdb_ids = \",\".join([p.strip() for p in pdb_raw.split(\",\")]) if pdb_raw else \"\"\n",
    "\n",
    "    p_cfg = prompts_cfg.get(\"prompts\", prompts_cfg)\n",
    "\n",
    "    use_chat_format = p_cfg.get(\"use_chat_format\", False)\n",
    "    system_prompt = p_cfg.get(\"system\", \"\")\n",
    "    user_template = select_user_template(p_cfg, test=test, tier=tier)\n",
    "\n",
    "    if use_chat_format:\n",
    "        prompt = compose_chat_prompt(\n",
    "            drug=drug,\n",
    "            receptor=receptor,\n",
    "            pdb_ids=pdb_ids,\n",
    "            system_prompt=system_prompt,\n",
    "            user_template=user_template,\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "    else:\n",
    "        prompt = compose_plain_prompt(\n",
    "            drug=drug,\n",
    "            receptor=receptor,\n",
    "            pdb_ids=pdb_ids,\n",
    "            user_template=user_template,\n",
    "        )\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    gen_cfg = infer_cfg[\"generation\"]\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=gen_cfg[\"max_new_tokens\"],\n",
    "        do_sample=gen_cfg[\"do_sample\"],\n",
    "        temperature=gen_cfg[\"temperature\"],\n",
    "        top_p=gen_cfg[\"top_p\"],\n",
    "        top_k=gen_cfg[\"top_k\"],\n",
    "        repetition_penalty=gen_cfg[\"repetition_penalty\"],\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "    # Decode only the newly generated portion\n",
    "    generated = out[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    text = tokenizer.decode(generated, skip_special_tokens=True)\n",
    "\n",
    "    # Optional: apply custom stop strings\n",
    "    for s in gen_cfg.get(\"stop_strings\", []):\n",
    "        if s and s in text:\n",
    "            text = text.split(s)[0].rstrip()\n",
    "            break\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Output path helper\n",
    "# -------------------------------\n",
    "\n",
    "def get_output_path(\n",
    "    infer_cfg: Dict[str, Any],\n",
    "    test: bool,\n",
    "    relation: bool,\n",
    "    real_world: bool,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Decide which CSV path to save to based on flags.\n",
    "    Uses the `save_paths` block from validation_params.yaml.\n",
    "    \"\"\"\n",
    "    paths = infer_cfg[\"save_paths\"]\n",
    "\n",
    "    if not test:\n",
    "        return paths[\"inference_reports\"]\n",
    "\n",
    "    if real_world:\n",
    "        return paths[\"real_world_test\"]\n",
    "\n",
    "    if relation:\n",
    "        return paths[\"some_relation\"]\n",
    "\n",
    "    return paths[\"no_relation\"]\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# DataFrame-level inference\n",
    "# -------------------------------\n",
    "\n",
    "def run_inference(\n",
    "    df: pd.DataFrame,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    infer_cfg: Dict[str, Any],\n",
    "    prompts_cfg: Dict[str, Any],\n",
    "    test: bool,\n",
    "    relation: bool,\n",
    "    real_world: bool,\n",
    "    tier: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run inference over all rows in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df: Input examples (must contain columns specified in infer_cfg[\"columns\"]).\n",
    "        model, tokenizer: Loaded via `load_model_and_tokenizer_for_inference`.\n",
    "        infer_cfg: validation_params.yaml contents.\n",
    "        prompts_cfg: prompts.yaml contents.\n",
    "        test: If False => validation-like set; if True => test scenarios.\n",
    "        relation: If True (and test=True), saves to \"some_relation\" path.\n",
    "        real_world: If True (and test=True), saves to \"real_world_test\" path.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with an extra column `generated_report`.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    for _, row in df.iterrows():\n",
    "        report = generate_for_row(\n",
    "            row=row.to_dict(),\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            infer_cfg=infer_cfg,\n",
    "            prompts_cfg=prompts_cfg,\n",
    "            test=test,\n",
    "            tier=tier,\n",
    "        )\n",
    "        rec = dict(row)\n",
    "        rec[\"generated_report\"] = report\n",
    "        rec[\"prompt_tier\"] = tier\n",
    "        records.append(rec)\n",
    "\n",
    "    out_df = pd.DataFrame(records)\n",
    "    out_path = get_output_path(\n",
    "        infer_cfg=infer_cfg,\n",
    "        test=test,\n",
    "        relation=relation,\n",
    "        real_world=real_world,\n",
    "    )\n",
    "    print(out_path)\n",
    "    out_path.replace(\".csv\", f\"_{tier}.csv\")\n",
    "    out_df.to_csv(out_path, index=False)\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e2ba7c",
   "metadata": {},
   "source": [
    "## basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24a8bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_model_and_tokenizer_for_inference(PARAMS)\n",
    "test_outputs = run_inference(\n",
    "    df=test_df,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    infer_cfg=PARAMS,\n",
    "    prompts_cfg=prompts,\n",
    "    test=True,\n",
    "    relation=False,\n",
    "    real_world=True,\n",
    "    tier=\"basic\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3f16b5",
   "metadata": {},
   "source": [
    "## onco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70622548",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_outputs = run_inference(\n",
    "    df=test_df,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    infer_cfg=PARAMS,\n",
    "    prompts_cfg=prompts,\n",
    "    test=True,\n",
    "    relation=False,\n",
    "    real_world=True,\n",
    "    tier=\"oncologist\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1a1f6a",
   "metadata": {},
   "source": [
    "## review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5514c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_outputs = run_inference(\n",
    "    df=test_df,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    infer_cfg=PARAMS,\n",
    "    prompts_cfg=prompts,\n",
    "    test=True,\n",
    "    relation=False,\n",
    "    real_world=True,\n",
    "    tier=\"reviewer\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
